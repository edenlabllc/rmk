{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RMK CLI - Reduced Management for Kubernetes","text":"<p>Command-line tool for simplified management and provisioning of Kubernetes clusters and environments, Helm secrets and releases, built according to best practices in CI/CD and DevOps.</p> <ul> <li>Overview</li> <li>Advantages</li> <li>Supported Kubernetes cluster providers<ul> <li>Provisioned by RMK</li> <li>Provisioned using third-party tools and services</li> </ul> </li> <li>Edenlab LLC use cases<ul> <li>Efficiency in numbers</li> <li>Managing clusters</li> <li>Related repositories<ul> <li>GitHub</li> <li>Helm charts</li> </ul> </li> </ul> </li> <li>Requirements<ul> <li>Operating systems (OS)</li> <li>Software</li> </ul> </li> <li>Installation</li> <li>Update<ul> <li>General update process</li> <li>Updating to specific version</li> </ul> </li> <li>Quickstart</li> <li>Backward compatibility</li> <li>Configuration<ul> <li>Configuration management<ul> <li>Initialization of AWS cluster provider</li> <li>Initialization of Azure cluster provider</li> <li>Initialization of GCP cluster provider</li> <li>Initialization of K3D cluster provider</li> <li>Initialization of On-Premise cluster provider</li> </ul> </li> <li>Project management<ul> <li>Requirement for project repository</li> <li>Preparation of project repository</li> <li>Dependencies management and project inheritance</li> </ul> </li> <li>Cluster management<ul> <li>Exported environment variables</li> <li>Using AWS cluster provider</li> <li>Using Azure cluster provider</li> <li>Using GCP cluster provider</li> <li>Using K3D cluster provider</li> <li>Using On-Premise cluster provider</li> </ul> </li> <li>Release management</li> <li>Secrets management</li> </ul> </li> <li>Commands</li> <li>Roadmap</li> <li>Development and release</li> <li>License</li> <li>Code of Conduct</li> </ul>"},{"location":"#overview","title":"Overview","text":"<p>RMK stands for \u201cReduced Management for Kubernetes\u201d.</p> <p>The main goal of the CLI tool is to simplify (reduce) the management of Kubernetes clusters and releases, serving as a \u201cSwiss knife\u201d for daily CI/CD and DevOps tasks while allowing efficient control with a minimal set of CLI commands.</p> <p>RMK serves as a wrapper for various popular CI/CD and DevOps CLI tools, including:</p> <ul> <li>Helmfile</li> <li>Helm</li> <li>kubectl</li> <li>clusterctl</li> <li>K3D</li> <li>SOPS</li> <li>Age</li> </ul> <p>It leverages Kubernetes Cluster API for cluster provisioning and management across different environments, such as cloud providers and on-premise deployments.</p> <p>RMK has been designed to be used by different IT specialists, among them are DevOps engineers, software developers, SREs, cloud architects, system analytics, software testers and even managers with minimal technical background.</p>"},{"location":"#advantages","title":"Advantages","text":"<p>RMK simplifies the setup and management of Kubernetes-based projects of any complexity due to the following advantages:</p> <ul> <li>Time-proven CI/CD solution: Tested and validated across multiple cloud providers and   real customers, RMK leverages Kubernetes Cluster API for cluster provisioning   and Helmfile/Helm for efficient release and secrets   management.</li> <li>Seamless integration with CI/CD platforms: A   self-sufficient, portable binary that follows the 12-factor app methodology and can   easily be integrated with any CI/CD solution.</li> <li>Built-in versioning for CI/CD pipelines: Supports static and   dynamic tags (e.g., SemVer2) for project and releases to guarantee stable, well-tested, and   predictable deployments.</li> <li>Transparent project structure and   dependency management:   Enables rapid project setup and hierarchical project inheritance, e.g., \u201cparent-child\u201d or \u201cupstream-downstream\u201d   relationships between sibling projects to enable release configuration reuse.</li> <li>Batch secret   management: Automates templating, generation, and encryption of secrets across all environments   in batch mode.</li> <li>Adheres to the GitOps approach: Uses Git branches as unique identifiers for   environments, clusters, configurations, and project management in Kubernetes.</li> <li>Follows the GitLab Flow model: Implements   a standard branching strategy (<code>develop</code>, <code>staging</code>, <code>production</code>) and ephemeral branches (<code>feature/*</code>, <code>release/*</code>,   <code>hotfix/*</code>) for   temporary environments.</li> <li>Aligns with the DevOps methodology: Enables multiple teams to develop and   release independently while seamlessly integrating their work into a single project.</li> <li>Directly executes the wrapped CLI tools: Calls CLI tools as a user would, passing the correct   arguments and flags   based on the project configuration, ensuring RMK updates remain decoupled from CLI tool updates for continued access   to new features and fixes.</li> </ul>"},{"location":"#supported-kubernetes-cluster-providers","title":"Supported Kubernetes cluster providers","text":""},{"location":"#provisioned-by-rmk","title":"Provisioned by RMK","text":"<p>RMK currently supports provisioning and management of the following Kubernetes clusters:</p> <ul> <li>Amazon Elastic Kubernetes Service (EKS)</li> <li>Azure Kubernetes Service (AKS)</li> <li>Google Kubernetes Engine (GKE)</li> <li>On-Premise</li> <li>Single-machine using K3D</li> </ul> <p>Please see the Roadmap section for more details on upcoming features.</p>"},{"location":"#provisioned-using-third-party-tools-and-services","title":"Provisioned using third-party tools and services","text":"<p>By design, RMK can work with any existing Kubernetes cluster, provided it has been provisioned in advance by a third party. The CLI tool simply requires an existing Kubernetes context to connect to and manage the cluster.</p>"},{"location":"#edenlab-llc-use-cases","title":"Edenlab LLC use cases","text":""},{"location":"#efficiency-in-numbers","title":"Efficiency in numbers","text":"<p>Initially, it has been developed by Edenlab LLC as the main CLI for provisioning and  management of Kodjin FHIR Server on Kubernetes clusters in different environments.</p> <p>Since 2021, RMK has been an integral part of the company\u2019s Kubernetes infrastructure, used regularly for automated provisioning and destroying temporary Kubernetes clusters for development and testing purposes, both manually and automatically within CI/CD pipelines.</p> <p> Proven at scale:</p> <ul> <li>220+ clusters handled monthly (based on a 5-day workweek).</li> <li>2,600+ clusters handled annually.</li> <li>12,000+ clusters orchestrated since 2021.</li> </ul> <p>Beyond internal use, RMK is also leveraged by various external clients to streamline their CI/CD workflows, ensuring fast and efficient Kubernetes environment management.</p>"},{"location":"#managing-clusters","title":"Managing clusters","text":"<p>At Edenlab LLC, RMK is utilized to deploy the Kodjin FHIR Server across various cloud providers and on-premise environments.</p> <p>Examples of Kubernetes providers where Kodjin has already been deployed include:</p> <ul> <li>Amazon Elastic Kubernetes Service (EKS)</li> <li>Azure Kubernetes Service (AKS)</li> <li>Google Kubernetes Engine (GKE)</li> <li>Open Telekom Cloud - Cloud Container Engine (CCE)</li> <li>Rancher Kubernetes Platform</li> <li>Kubermatic Kubernetes Platform (KKP)</li> <li>On-premise</li> <li>Single-machine using K3D</li> </ul> <p>A standard Kodjin-based cluster follows a 4-level inheritance structure:</p> <ul> <li>cluster-deps (upstream#1):   Provides Kubernetes Cluster API and essential system components required by RMK   for provisioning Kubernetes clusters across various providers.</li> <li>Dependencies (upstream#2):   Includes core components such as databases, search engines, caches, load balancers/proxies, and operators.   etc., uses cluster-deps as its primary project   dependency.</li> <li>Kodjin (downstream#1):   A set of Rust microservices that form the Kodjin FHIR   API (REST).</li> <li>Target project (tenant) (downstream#2):   Encompasses products built on top of Kodjin, including UI components, user portals, and middleware services, such as   the   e.g., Kodjin Demo FHIR Server</li> </ul> <p>Each project repository follows a standard GitLab Flow branching model.</p>"},{"location":"#related-repositories","title":"Related repositories","text":""},{"location":"#github","title":"GitHub","text":"<ul> <li>cluster-deps.bootstrap.infra:   Kubernetes Cluster API and system components required for provisioning of   Kubernetes clusters for different providers.</li> <li>helmfile.hooks.infra:   A collection of shell scripts used as Helmfile hooks in   dependencies, Kodjin, or any other project,   e.g.,   check cluster-deps global configuration.</li> <li>aws-iam-provisioner.operators.infra:   Kubernetes operator for automatic provisioning of IAM roles on the fly for the Kubernetes clusters managed   using Kubernetes Cluster API Provider AWS.</li> <li>ebs-snapshot-provision.operators.infra:   Kubernetes operator for automatic provisioning of Amazon EBS snapshots to be reused    in existing Kubernetes clusters.</li> <li>ecr-token-refresh.operators.infra:   Kubernetes operator for automatic refresh of the Amazon ECR authorization token    before it expires.</li> <li>on-premise-configurator.operators.infra:   Kubernetes operator for declarative configuration of remote bare-metal or virtual machines over SSH, for both   isolated and network-connected environments,   a fully compliant Kubernetes Cluster API   infrastructure provider.</li> <li>secrets-sync.operators.infra:   Kubernetes operator for automatically copying of existing Kubernetes secrets between namespaces.</li> </ul>"},{"location":"#helm-charts","title":"Helm charts","text":"<ul> <li>core-charts:   A publicly accessible, S3-based Helm chart repository used by Kodjin, or any other project, e.g.,   check cluster-deps Helmfile.</li> </ul>"},{"location":"#requirements","title":"Requirements","text":""},{"location":"#operating-systems-os","title":"Operating systems (OS)","text":"<p>Currently, RMK only supports Unix-like operating systems (OS):</p> <ul> <li>MacOS: amd64, arm64 (M series processors   require Rosetta)</li> <li>Linux: amd64</li> </ul>"},{"location":"#software","title":"Software","text":"<p>The following software is required to run RMK:</p> <ul> <li>Git</li> <li>[for local K3D clusters]:   Version v5.X.X   requires Docker &gt;=   v20.10.5 (runc &gt;= v1.0.0-rc93) to work properly.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install RMK, run the self-installer script using the following command:</p> <pre><code>curl -sL \"https://edenlabllc-rmk.s3.eu-north-1.amazonaws.com/rmk/s3-installer\" | bash\n</code></pre> <p>Alternatively, you can go directly to the releases and download the binary or build from source.</p>"},{"location":"#update","title":"Update","text":""},{"location":"#general-update-process","title":"General update process","text":"<p>To update RMK to the latest version, run the following command:</p> <pre><code>rmk update\n</code></pre>"},{"location":"#updating-to-specific-version","title":"Updating to specific version","text":"<p>You can update to a specific RMK version to maintain backward compatibility or when updating to the latest version is not possible.</p> <p>This may be necessary due to specific version requirements or when a bug has been detected.</p> <p>To update to a specific version, use the following command:</p> <pre><code>rmk update --version vX.X.X \n</code></pre>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li> Integration with Helmfile vals: Integrate RMK with a tool for advanced    values and secrets management.</li> <li> Integration with open-source AI models   for project generation:   Generate project structure, machine setup, and resource configuration directly from natural-language prompts.</li> <li> Enhanced automatic testing of RMK during the CI/CD pipeline:   Ensure that changes to the RMK codebase   do not introduce errors or regressions during the CI/CD across all cluster providers.</li> <li> Guidelines for contributors:   Create comprehensive guidelines and instructions for creating pull requests (PRs).</li> <li> Implementation of additional cloud Kubernetes Cluster API providers:   Implement support for other popular Kubernetes services such as   GKE,   AKS, etc.</li> <li> Implementation of on-premise Kubernetes Cluster API provider:   Implement support for provisioning and destroying remote bare-metal or virtual machine\u2013based Kubernetes clusters.</li> <li> Web documentation generation using MkDocs: Add an HTML documentation generator   based on the .md files.</li> </ul> <p>Please refer to GitHub issues for more information.</p>"},{"location":"#development-and-release","title":"Development and release","text":"<p>The guidelines are available at the link.</p>"},{"location":"#license","title":"License","text":"<p>RMK is open source software (OSS) licensed under the Apache 2.0 License.</p>"},{"location":"#code-of-conduct","title":"Code of Conduct","text":"<p>This project adheres to the Contributor Covenant \u0421ode of \u0421onduct. By participating, you are expected to uphold this code.</p> <p>Please refer to our Contributing Guidelines for further information.</p>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and maintainers pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others\u2019 private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Maintainers are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the maintainers responsible for enforcement and specified in CODEOWNERS.</p> <p>All complaints will be reviewed and investigated promptly and fairly.</p> <p>All maintainers are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Maintainers will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from maintainers, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla\u2019s code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"CONTRIBUTING/","title":"Contributing Guidelines","text":""},{"location":"CONTRIBUTING/#reporting-issues","title":"Reporting Issues","text":"<p>Before creating a new issue, please check first if a similar issue already exists or was recently closed.</p>"},{"location":"CONTRIBUTING/#contributing-code","title":"Contributing Code","text":"<p>Currently, we do not accept pull requests. Please report an issue instead.</p>"},{"location":"CONTRIBUTING/#code-review","title":"Code Review","text":"<p>Everyone is invited to review and comment on pull requests.</p>"},{"location":"SECURITY/","title":"Security Policy","text":""},{"location":"SECURITY/#supported-versions","title":"Supported Versions","text":"<p>We release patches for security vulnerabilities for latest software versions.</p>"},{"location":"SECURITY/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>The security team and community take all security vulnerabilities seriously. Thank you for improving the security of our open source software. We appreciate your efforts and responsible disclosure and will make every effort to acknowledge your contributions.</p> <p>Report security vulnerabilities by notifying the team specified in CODEOWNERS.</p> <p>The lead maintainers will acknowledge your email and send a more detailed response indicating the next steps in handling your report. After the initial reply to your report, the security team will endeavor to keep you informed of the progress towards a fix and full announcement, and may ask for additional information or guidance.</p> <p>Report security vulnerabilities in third-party modules to the person or team maintaining the module.</p>"},{"location":"SECURITY/#disclosure-policy","title":"Disclosure Policy","text":"<p>When the security team receives a security bug report, they will assign it to a primary handler. This person will coordinate the fix and release process, involving the following steps:</p> <ul> <li>Confirm the problem and determine the affected versions.</li> <li>Audit code to find any potential similar problems.</li> <li>Prepare fixes for all releases still under maintenance. These fixes will be released as fast as possible.</li> </ul>"},{"location":"SECURITY/#preferred-languages","title":"Preferred Languages","text":"<p>We prefer all communications to be in English.</p>"},{"location":"backward-compatibility/","title":"Backward compatibility","text":""},{"location":"backward-compatibility/#breaking-change-releases","title":"Breaking change releases","text":""},{"location":"backward-compatibility/#v0442-v0450","title":"v0.44.2 -&gt; v0.45.0","text":""},{"location":"backward-compatibility/#motivation","title":"Motivation","text":"<p>The main change in the v0.45.0 RMK version is the replacement of the technology stack for cluster provisioning, transitioning from Terraform to Kubernetes Cluster API.</p> <p>This change was driven by several key factors:</p> <ol> <li> <p>Maintaining open-source integrity:</p> <p>Terraform\u2019s transition to a BSL license conflicts with our   commitment to keeping RMK fully open-source (OSS).   By switching to Kubernetes Cluster API, we ensure that our customers\u2019 interests remain unaffected.</p> <p>More details on the Terraform\u2019s license change are available at the link.</p> </li> <li> <p>Kubernetes-native solution:</p> <p>We needed a provisioning approach that seamlessly integrates with Kubernetes across various environments.</p> <p>With the new version, we now support  AWS,  Azure,  GCP,  K3D (local installation).</p> <p>On-premise support is expected in upcoming releases.</p> </li> <li> <p>Simplified configuration management:</p> <p>Cluster configurations are now stored in Helm charts, aligning with the way  installed components are managed. This ensures a unified format for all declarations.</p> </li> <li> <p>Seamless cluster upgrades:</p> <p>Our new approach makes cluster updates Kubernetes-native with  pod status awareness and  zero-downtime upgrades.</p> </li> </ol> <p>This transition marks a significant step in enhancing RMK\u2019s provisioning capabilities, ensuring better scalability, openness, and ease of management. </p> <p>Stay tuned for more updates in upcoming releases! \ud83d\ude80</p>"},{"location":"backward-compatibility/#deprecated-features","title":"Deprecated features","text":"<p>For the rmk config init command:</p> <ul> <li>\u2013artifact-mode, \u2013aws-reconfigure-artifact-license: removed the flag along with the functionality, no longer   needed.</li> <li>\u2013aws-ecr-host, \u2013aws-ecr-region, \u2013aws-ecr-user-name: removed the flag along with the functionality,   replaced by the third-party Kubernetes-native   ecr-token-refresh operator.</li> <li>\u2013aws-reconfigure: removed the flag, replaced AWS CLI with   AWS SDK.</li> <li>\u2013cloudflare-token: removed the flag along with the functionality, replaced by the third-party   Kubernetes-native external-dns operator.</li> <li>\u2013cluster-provisioner-state-locking: removed the flag (Terraform is no longer in use).</li> <li>\u2013config-from-environment: removed the flag along with the functionality.</li> <li>\u2013root-domain: removed the flag, replaced by the declarative configuration   via project.yaml.</li> <li>\u2013s3-charts-repo-region: removed the flag, replaced with the private repository configuration   via Helmfile.</li> </ul> <p>For the rmk cluster command category:</p> <ul> <li>container-registry: removed the command along with all flags.</li> <li>destroy: removed the command along with all flags (Terraform is no longer in use).</li> <li>list: removed the command along with all flags (Terraform is no longer in use).</li> <li>provision: removed the command along with all flags (Terraform is no longer in use).</li> <li>state: removed the command along with all flags (Terraform is no longer in use).</li> </ul>"},{"location":"backward-compatibility/#steps-to-migrate","title":"Steps to migrate","text":""},{"location":"backward-compatibility/#newly-created-project-repositories","title":"Newly created project repositories","text":"<p>Before performing actions via RMK with this project repository, simply update to the v0.45.0 version.</p> <pre><code>rmk update\n</code></pre>"},{"location":"backward-compatibility/#previously-created-project-repositories-for-the-aws-cluster-provider","title":"Previously created project repositories for the AWS cluster provider","text":"<p>To ensure a successful migration, the following the steps should be executed in the specified order:</p> <ol> <li> <p>Ensure you are using the previous v0.44.2 RMK version.</p> <pre><code>rmk update -v v0.44.2\n</code></pre> </li> <li> <p>Download private SOPS Age keys of the current    RMK version if you haven\u2019t done it earlier.</p> <pre><code>rmk config init --github-token=&lt;github_personal_access_token&gt;\n</code></pre> <p>Skip this step if you lack administrator permissions for the selected AWS account.</p> </li> <li> <p>Save the old path to the private SOPS Age keys storage directory to an environment variable.</p> <pre><code>RMK_SOPS_AGE_KEYS_PATH_OLD=\"$(rmk --log-format=json config view | yq '.config.SopsAgeKeys')\"\n</code></pre> <p>Skip this step if you lack administrator permissions for the selected AWS account.</p> </li> <li> <p>Update your current version to v0.45.0.</p> <pre><code>rmk update\n</code></pre> </li> <li> <p>Add root domain specification    in project.yaml for project    repository.</p> <pre><code>project:\n  # ...\n  spec:\n    environments:\n      develop:\n        root-domain: &lt;custom_root_domain_name&gt; # or \"*.edenlab.dev\" for the Edenlab team\n      production:\n        root-domain: &lt;custom_root_domain_name&gt; # or \"*.edenlab.dev\" for the Edenlab team\n      staging:\n        root-domain: &lt;custom_root_domain_name&gt; # or \"*.edenlab.dev\" for the Edenlab team\n</code></pre> <p>If the project.yaml file has the <code>spec.environments</code> section of a deprecated format already, e.g.:</p> <pre><code>project:\n # ...\n spec:\n   environments:\n     - develop # deprecated\n     - production # deprecated\n     - staging # deprecated\n</code></pre> <p>be sure to replace the scalar strings with the new <code>spec.environments</code> objects containing the respective root  domains.</p> </li> <li> <p>Initialize a new configuration    specifying the AWS cluster provider.</p> <pre><code>rmk config init --cluster-provider=aws \\\n    --aws-access-key-id=&lt;aws_access_key_id&gt; \\\n    --aws-region=&lt;aws_region&gt; \\\n    --aws-secret-access-key=&lt;aws_secret_access_key&gt; \\\n    --github-token=&lt;github_personal_access_token&gt;\n</code></pre> </li> <li> <p>Copy private SOPS Age keys from the old path    to the new directory.</p> <pre><code>cp -f \"${RMK_SOPS_AGE_KEYS_PATH_OLD}\"/* \"$(rmk --log-format=json config view | yq '.config.SopsAgeKeys')\"\nunset RMK_SOPS_AGE_KEYS_PATH_OLD\n</code></pre> <p>Skip this step if you lack administrator permissions for the selected AWS account.</p> </li> <li> <p>Upload the old private SOPS Age keys to AWS Secret Manager.</p> <pre><code>rmk secret keys upload\n</code></pre> <p>Skip this step if you lack administrator permissions for the selected AWS account.</p> </li> <li> <p>Migrate the code to the new repository structure, for example, compatible with cluster provisioning using Kubernetes Cluster API Provider AWS.</p> </li> </ol>"},{"location":"commands/","title":"NAME","text":"<p>RMK CLI - Reduced management for Kubernetes</p>"},{"location":"commands/#synopsis","title":"SYNOPSIS","text":"<p>rmk</p> <pre><code>[--help|-h]\n[--log-format|--lf]=[value]\n[--log-level|--ll]=[value]\n[--version|-v]\n</code></pre>"},{"location":"commands/#description","title":"DESCRIPTION","text":"<p>Command line tool for reduced management of the provision of Kubernetes clusters in different environments and management of service releases.</p> <p>BuiltBy: goreleaser  Commit: 92a29b7  Date: 2025-09-22T07:32:29Z  Target: linux_amd64</p> <p>Usage:</p> <pre><code>rmk [GLOBAL OPTIONS] command [COMMAND OPTIONS] [ARGUMENTS...]\n</code></pre>"},{"location":"commands/#global-options","title":"GLOBAL OPTIONS","text":"<p>\u2013help, -h: show help</p> <p>\u2013log-format, \u2013lf=\u201d\u201c: log output format, available: console, json (default: \u201cconsole\u201d)</p> <p>\u2013log-level, \u2013ll=\u201d\u201c: log level severity, available: debug, info, error (default: \u201cinfo\u201d)</p> <p>\u2013version, -v: print the version</p>"},{"location":"commands/#commands","title":"COMMANDS","text":""},{"location":"commands/#cluster","title":"cluster","text":"<p>Cluster management</p>"},{"location":"commands/#capi-c","title":"capi, c","text":"<p>CAPI cluster management</p>"},{"location":"commands/#create-c","title":"create, c","text":"<p>Create CAPI management cluster</p>"},{"location":"commands/#delete-d","title":"delete, d","text":"<p>Delete CAPI management cluster</p>"},{"location":"commands/#destroy","title":"destroy","text":"<p>Destroy K8S target (workload) cluster</p>"},{"location":"commands/#list-l","title":"list, l","text":"<p>List CAPI management clusters</p>"},{"location":"commands/#provision-p","title":"provision, p","text":"<p>Provision K8S target (workload) cluster</p>"},{"location":"commands/#update-u","title":"update, u","text":"<p>Update CAPI management cluster</p>"},{"location":"commands/#k3d-k","title":"k3d, k","text":"<p>K3D cluster management</p>"},{"location":"commands/#create-c_1","title":"create, c","text":"<p>Create K3D cluster</p> <p>\u2013k3d-volume-host-path, \u2013kv=\u201d\u201c: host local directory path for mount into K3D cluster (default: present working directory)</p>"},{"location":"commands/#delete-d_1","title":"delete, d","text":"<p>Delete K3D cluster</p>"},{"location":"commands/#import-i","title":"import, i","text":"<p>Import images from docker to K3D cluster</p> <p>\u2013k3d-import-image, \u2013ki=\u201d\u201c: list of images to import into running K3D cluster</p>"},{"location":"commands/#list-l_1","title":"list, l","text":"<p>List K3D clusters</p>"},{"location":"commands/#start-s","title":"start, s","text":"<p>Start K3D cluster</p>"},{"location":"commands/#stop","title":"stop","text":"<p>Stop K3D cluster</p>"},{"location":"commands/#switch-s","title":"switch, s","text":"<p>Switch Kubernetes context to project cluster</p> <p>\u2013force, -f: force update Kubernetes context from remote cluster</p>"},{"location":"commands/#completion","title":"completion","text":"<p>Completion management</p>"},{"location":"commands/#zsh-z","title":"zsh, z","text":"<p>View Zsh completion scripts</p>"},{"location":"commands/#config","title":"config","text":"<p>Configuration management</p>"},{"location":"commands/#init-i","title":"init, i","text":"<p>Initialize configuration for current project and selected environment</p> <p>\u2013aws-access-key-id, \u2013awid=\u201d\u201c: AWS access key ID for IAM user</p> <p>\u2013aws-region, \u2013awr=\u201d\u201c: AWS region for current AWS account</p> <p>\u2013aws-secret-access-key, \u2013awsk=\u201d\u201c: AWS secret access key for IAM user</p> <p>\u2013aws-session-token, \u2013awst=\u201d\u201c: AWS session token for IAM user</p> <p>\u2013azure-client-id, \u2013azid=\u201d\u201c: Azure client ID for Service Principal</p> <p>\u2013azure-client-secret, \u2013azp=\u201d\u201c: Azure client secret for Service Principal</p> <p>\u2013azure-key-vault-resource-group-name, \u2013azkvrg=\u201d\u201c: Azure Key Vault custom resource group name</p> <p>\u2013azure-location, \u2013azl=\u201d\u201c: Azure location</p> <p>\u2013azure-service-principle, \u2013azsp: Azure service principal STDIN content</p> <p>\u2013azure-subscription-id, \u2013azs=\u201d\u201c: Azure subscription ID for current platform domain</p> <p>\u2013azure-tenant-id, \u2013azt=\u201d\u201c: Azure tenant ID for Service Principal</p> <p>\u2013cluster-provider, \u2013cp=\u201d\u201c: cluster provider for provisioning (default: \u201ck3d\u201d)</p> <p>\u2013gcp-region, \u2013gr=\u201d\u201c: GCP region</p> <p>\u2013github-token, \u2013ght=\u201d\u201c: GitHub personal access token, required when using private repositories</p> <p>\u2013google-application-credentials, \u2013gac=\u201d\u201c: path to GCP service account credentials JSON file</p> <p>\u2013onprem-ssh-init-server-host, \u2013opsish=\u201d\u201c: K3S init server host used to retrieve kubeconfig via SSH</p> <p>\u2013onprem-ssh-private-key, \u2013opspk=\u201d\u201c: path to SSH private key. If not set, RMK will search in default SSH locations (e.g., ~/.ssh/id_[ed25519|rsa|ecdsa|dsa])</p> <p>\u2013onprem-ssh-user, \u2013opsu=\u201d\u201c: username to use for SSH authentication</p> <p>\u2013progress-bar, -p: globally disable or enable progress bar for download process</p> <p>\u2013slack-channel, \u2013sc=\u201d\u201c: channel name for Slack notifications</p> <p>\u2013slack-message-details, \u2013smd=\u201d\u201c: additional details for Slack message body</p> <p>\u2013slack-notifications, -n: enable Slack notifications</p> <p>\u2013slack-webhook, \u2013sw=\u201d\u201c: URL for Slack webhook</p>"},{"location":"commands/#delete-d_2","title":"delete, d","text":"<p>Delete configuration for selected environment</p>"},{"location":"commands/#list-l_2","title":"list, l","text":"<p>List available configurations for current project</p> <p>\u2013all, -a: list all project configurations</p>"},{"location":"commands/#view-v","title":"view, v","text":"<p>View configuration for selected environment</p>"},{"location":"commands/#doc","title":"doc","text":"<p>Documentation management</p> <p>\u2013help, -h: show help</p>"},{"location":"commands/#generate-g","title":"generate, g","text":"<p>Generate documentation by commands and flags in Markdown format</p> <p>\u2013help, -h: show help</p>"},{"location":"commands/#help-h","title":"help, h","text":"<p>Shows a list of commands or help for one command</p>"},{"location":"commands/#help-h_1","title":"help, h","text":"<p>Shows a list of commands or help for one command</p>"},{"location":"commands/#project","title":"project","text":"<p>Project management</p>"},{"location":"commands/#generate-g_1","title":"generate, g","text":"<p>Generate project directories and files structure</p> <p>\u2013create-sops-age-keys, -c: create SOPS age keys for generated project structure</p> <p>\u2013environment, -e=\u201d\u201c: list of project environments, root-domain config option must be provided: .root-domain= <p>\u2013owner, -o=\u201d\u201c: list of project owners</p> <p>\u2013scope, -s=\u201d\u201c: list of project scopes</p>"},{"location":"commands/#update-u_1","title":"update, u","text":"<p>Update project file with specific dependencies version</p> <p>\u2013dependency, -d=\u201d\u201c: specific dependency name for updating project file</p> <p>\u2013skip-ci, -i: add [skip ci] to commit message line to skip triggering other CI builds</p> <p>\u2013skip-commit, -c: only change a version in for project file without committing and pushing it</p> <p>\u2013version, -v=\u201d\u201c: specific dependency version for updating project file</p>"},{"location":"commands/#release","title":"release","text":"<p>Release components list from state file (Helmfile)</p>"},{"location":"commands/#build-b","title":"build, b","text":"<p>Build releases</p> <p>\u2013helmfile-args, \u2013ha=\u201d\u201c: Helmfile additional arguments</p> <p>\u2013helmfile-log-level, \u2013hll=\u201d\u201c: Helmfile log level severity, available: debug, info, warn, error (default: \u201cerror\u201d)</p> <p>\u2013selector, -l=\u201d\u201c: list of release labels, used as selector, selector can take form of foo=bar or foo!=bar</p> <p>\u2013skip-context-switch, -s: skip context switch for not provisioned cluster</p>"},{"location":"commands/#destroy-d","title":"destroy, d","text":"<p>Destroy releases</p> <p>\u2013helmfile-args, \u2013ha=\u201d\u201c: Helmfile additional arguments</p> <p>\u2013helmfile-log-level, \u2013hll=\u201d\u201c: Helmfile log level severity, available: debug, info, warn, error (default: \u201cerror\u201d)</p> <p>\u2013selector, -l=\u201d\u201c: list of release labels, used as selector, selector can take form of foo=bar or foo!=bar</p> <p>\u2013skip-context-switch, -s: skip context switch for not provisioned cluster</p>"},{"location":"commands/#list-l_3","title":"list, l","text":"<p>List releases</p> <p>\u2013helmfile-args, \u2013ha=\u201d\u201c: Helmfile additional arguments</p> <p>\u2013helmfile-log-level, \u2013hll=\u201d\u201c: Helmfile log level severity, available: debug, info, warn, error (default: \u201cerror\u201d)</p> <p>\u2013output, -o=\u201d\u201c: output format, available: short, yaml (default: \u201cshort\u201d)</p> <p>\u2013selector, -l=\u201d\u201c: list of release labels, used as selector, selector can take form of foo=bar or foo!=bar</p> <p>\u2013skip-context-switch, -s: skip context switch for not provisioned cluster</p>"},{"location":"commands/#rollback-r","title":"rollback, r","text":"<p>Rollback specific releases to latest stable state</p> <p>\u2013release-name, \u2013rn=\u201d\u201c: list release names for rollback status in Kubernetes</p> <p>\u2013skip-context-switch, -s: skip context switch for not provisioned cluster</p>"},{"location":"commands/#sync-s","title":"sync, s","text":"<p>Sync releases</p> <p>\u2013helmfile-args, \u2013ha=\u201d\u201c: Helmfile additional arguments</p> <p>\u2013helmfile-log-level, \u2013hll=\u201d\u201c: Helmfile log level severity, available: debug, info, warn, error (default: \u201cerror\u201d)</p> <p>\u2013selector, -l=\u201d\u201c: list of release labels, used as selector, selector can take form of foo=bar or foo!=bar</p> <p>\u2013skip-context-switch, -s: skip context switch for not provisioned cluster</p>"},{"location":"commands/#template-t","title":"template, t","text":"<p>Template releases</p> <p>\u2013helmfile-args, \u2013ha=\u201d\u201c: Helmfile additional arguments</p> <p>\u2013helmfile-log-level, \u2013hll=\u201d\u201c: Helmfile log level severity, available: debug, info, warn, error (default: \u201cerror\u201d)</p> <p>\u2013selector, -l=\u201d\u201c: list of release labels, used as selector, selector can take form of foo=bar or foo!=bar</p> <p>\u2013skip-context-switch, -s: skip context switch for not provisioned cluster</p>"},{"location":"commands/#update-u_2","title":"update, u","text":"<p>Update releases file with specific environment values</p> <p>\u2013commit, -c: only commit and push changes for releases file</p> <p>\u2013deploy, -d: deploy updated releases after committed and pushed changes</p> <p>\u2013repository, -r=\u201d\u201c: specific repository for updating releases file</p> <p>\u2013skip-ci, -i: add [skip ci] to commit message line to skip triggering other CI builds</p> <p>\u2013skip-context-switch, -s: skip context switch for not provisioned cluster</p> <p>\u2013tag, -t=\u201d\u201c: specific tag for updating releases file</p>"},{"location":"commands/#secret","title":"secret","text":"<p>secrets management</p>"},{"location":"commands/#manager-m","title":"manager, m","text":"<p>batch secrets management</p>"},{"location":"commands/#decrypt-d","title":"decrypt, d","text":"<p>Decrypt secrets batch for selected scope and environment</p> <p>\u2013environment, -e=\u201d\u201c: list of secret environments, used as selector</p> <p>\u2013scope, -s=\u201d\u201c: list of secret scopes, used as selector</p>"},{"location":"commands/#encrypt-e","title":"encrypt, e","text":"<p>Encrypt secrets batch for selected scope and environment</p> <p>\u2013environment, -e=\u201d\u201c: list of secret environments, used as selector</p> <p>\u2013scope, -s=\u201d\u201c: list of secret scopes, used as selector</p>"},{"location":"commands/#generate-g_2","title":"generate, g","text":"<p>Generate secrets batch for selected scope and environment</p> <p>\u2013environment, -e=\u201d\u201c: list of secret environments, used as selector</p> <p>\u2013force, -f: force overwriting current secrets after generating new</p> <p>\u2013scope, -s=\u201d\u201c: list of secret scopes, used as selector</p>"},{"location":"commands/#keys-k","title":"keys, k","text":"<p>SOPS age keys management</p>"},{"location":"commands/#create-c_2","title":"create, c","text":"<p>Create SOPS age keys</p>"},{"location":"commands/#download-d","title":"download, d","text":"<p>Download SOPS age keys from S3 bucket</p>"},{"location":"commands/#upload-u","title":"upload, u","text":"<p>Upload SOPS age keys to S3 bucket</p>"},{"location":"commands/#encrypt-e_1","title":"encrypt, e","text":"<p>Encrypt secret file</p>"},{"location":"commands/#decrypt-d_1","title":"decrypt, d","text":"<p>Decrypt secret file</p>"},{"location":"commands/#view-v_1","title":"view, v","text":"<p>View secret file</p>"},{"location":"commands/#edit","title":"edit","text":"<p>Edit secret file</p>"},{"location":"commands/#update","title":"update","text":"<p>Update RMK CLI to a new version</p> <p>\u2013release-candidate, -r: force update RMK to latest release candidate version</p> <p>\u2013version, -v=\u201d\u201c: RMK special version (default: empty value corresponds latest version)</p>"},{"location":"commands/#help-h_2","title":"help, h","text":"<p>Shows a list of commands or help for one command</p>"},{"location":"development-and-release/","title":"Development and release","text":""},{"location":"development-and-release/#development","title":"Development","text":""},{"location":"development-and-release/#requirements-for-the-availability-of-tools-during-development","title":"Requirements for the availability of tools during development","text":"<ul> <li>Golang = v1.22.X</li> <li>GoReleaser = v1.23.0</li> </ul>"},{"location":"development-and-release/#building-from-source","title":"Building from source","text":"<p>To build RMK from source, run the following GoReleaser command from the root of the repository:</p> <pre><code>goreleaser build --snapshot --clean\n</code></pre> <p>You can also use this command for recompilation of RMK during development.</p>"},{"location":"development-and-release/#git-workflow","title":"Git workflow","text":"<p>In RMK development, we use the classic GitFlow workflow,  embracing all its advantages and disadvantages.</p> <p>The Git branch naming conventions are:</p> <ul> <li><code>feature/&lt;issue_key&gt;-&lt;issue_number&gt;-&lt;issue_description&gt;</code></li> <li><code>release/&lt;SemVer2&gt;</code></li> <li><code>hotfix/&lt;SemVer2&gt;</code></li> </ul> <p>For example:</p> <ul> <li><code>feature/RMK-123-add-some-important-feature</code></li> <li><code>release/v0.46.0</code></li> <li><code>hotfix/v0.46.1</code></li> </ul>"},{"location":"development-and-release/#release","title":"Release","text":"<p>After accumulating a certain set of features in the develop branch,  a <code>release/&lt;SemVer2&gt;</code> branch is created for the next release version.  Then a pull request (PR) is made from the <code>release/&lt;SemVer2&gt;</code> branch to the <code>master</code> branch.  This triggers a CI process that will build and release an intermediate version,  the <code>&lt;SemVer2&gt;-rc</code> release candidate. </p> <p>This version is available for update from RMK via the <code>rmk update --release-candidate</code> command and can be used for an intermediate or beta testing. </p> <p>After successful testing, the PR is merged into the <code>master</code> branch,  triggering another CI process that will release the stable RMK version. </p> <p>All CI processes are performed using GoReleaser, including the publishing of artifacts  for both the release and intermediate RMK versions.</p>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#overview","title":"Overview","text":"<p>This guide demonstrates how to use RMK to prepare the structure of a new project, create a local cluster based on K3D, and deploy your first application (Nginx) using Helmfile releases.</p> <p>All of this will be done in just 5 steps.</p>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Installed RMK.</li> <li>Fulfilled requirements   and prerequisites for proper   RMK operation.</li> </ul> <p>This example assumes the project (tenant) name is <code>rmk-test</code>, the current Git branch is <code>develop</code>, the configured Git remote is <code>origin</code>.</p>"},{"location":"quickstart/#steps","title":"Steps","text":"<ol> <li> <p>Generate    the project structure,    the project.yaml file, and SOPS    Age keys:</p> <pre><code>rmk project generate --scope rmk-test --environment \"develop.root-domain=localhost\" --create-sops-age-keys\n</code></pre> <p>The default scope is deps,  it is added unconditionally during the project generation process, no need to specify it explicitly.</p> Example output <pre><code>2025-01-29T14:20:02.954+0100  INFO    file /home/user/rmk-test/etc/deps/develop/values/aws-cluster.yaml.gotmpl generated\n2025-01-29T14:20:02.956+0100  INFO    file /home/user/rmk-test/etc/deps/develop/values/azure-cluster.yaml.gotmpl generated\n2025-01-29T14:20:02.956+0100  INFO    file /home/user/rmk-test/etc/deps/develop/values/gcp-cluster.yaml.gotmpl generated\n2025-01-29T14:20:02.957+0100  INFO    file /home/user/rmk-test/etc/deps/develop/globals.yaml.gotmpl generated\n2025-01-29T14:20:02.957+0100  INFO    file /home/user/rmk-test/etc/deps/develop/releases.yaml generated\n2025-01-29T14:20:02.957+0100  INFO    file /home/user/rmk-test/etc/deps/develop/secrets/.spec.yaml.gotmpl generated\n2025-01-29T14:20:02.957+0100  INFO    file /home/user/rmk-test/etc/deps/develop/secrets/.sops.yaml generated\n2025-01-29T14:20:02.957+0100  INFO    file /home/user/rmk-test/etc/rmk-test/develop/globals.yaml.gotmpl generated\n2025-01-29T14:20:02.958+0100  INFO    file /home/user/rmk-test/etc/rmk-test/develop/releases.yaml generated\n2025-01-29T14:20:02.958+0100  INFO    file /home/user/rmk-test/etc/rmk-test/develop/secrets/.spec.yaml.gotmpl generated\n2025-01-29T14:20:02.958+0100  INFO    file /home/user/rmk-test/etc/rmk-test/develop/values/rmk-test-app.yaml.gotmpl generated\n2025-01-29T14:20:02.958+0100  INFO    file /home/user/rmk-test/etc/rmk-test/develop/secrets/.sops.yaml generated\n2025-01-29T14:20:02.958+0100  INFO    file /home/user/rmk-test/.gitignore generated\n2025-01-29T14:20:02.959+0100  INFO    file /home/user/rmk-test/helmfile.yaml.gotmpl generated\n2025-01-29T14:20:02.959+0100  INFO    file /home/user/rmk-test/README.md generated\n2025-01-29T14:20:02.986+0100  INFO    generate age key for scope: deps\n2025-01-29T14:20:02.986+0100  INFO    update SOPS config file: /home/user/rmk-test/etc/deps/develop/secrets/.sops.yaml\n2025-01-29T14:20:03.000+0100  INFO    generate age key for scope: rmk-test\n2025-01-29T14:20:03.001+0100  INFO    update SOPS config file: /home/user/rmk-test/etc/rmk-test/develop/secrets/.sops.yaml\n</code></pre> </li> <li> <p>Initialize RMK configuration    for the repository:</p> <pre><code>rmk config init\n</code></pre> <p>The default cluster provider is <code>k3d</code>.</p> Example output <pre><code>2025-01-29T14:22:44.548+0100  INFO    loaded config file by path: /home/user/.rmk/config/rmk-test-develop.yaml\n2025-01-29T14:22:44.550+0100  INFO    RMK will use values for develop environment\n2025-01-29T14:22:44.553+0100  INFO    starting package download: cluster-deps.bootstrap.infra-v0.1.0\n2025-01-29T14:22:45.790+0100  INFO    downloaded: cluster-deps.bootstrap.infra-v0.1.0\n2025-01-29T14:22:45.793+0100  INFO    starting package download: helmfile.hooks.infra-v1.29.1\n2025-01-29T14:22:46.598+0100  INFO    downloaded: helmfile.hooks.infra-v1.29.1\n2025-01-29T14:22:46.864+0100  INFO    time spent on initialization: 2s\n</code></pre> </li> <li> <p>Create a local K3D cluster:</p> <pre><code>rmk cluster k3d create\n</code></pre> <p>Ensure that Docker is running.</p> Example output <pre><code>INFO[0000] Using config file /var/folders/_d/y2s0znsj5l117xk90392xc540000gn/T/k3d-config.51481123.yaml (k3d.io/v1alpha5#simple)\nINFO[0000] portmapping '8080:80' targets the loadbalancer: defaulting to [servers:*:proxy agents:*:proxy]\nINFO[0000] portmapping '8443:443' targets the loadbalancer: defaulting to [servers:*:proxy agents:*:proxy]\nINFO[0000] portmapping '9111:9000' targets the loadbalancer: defaulting to [servers:*:proxy agents:*:proxy]\nINFO[0000] Prep: Network\nINFO[0000] Created network 'k3d-rmk-test-develop'\nINFO[0000] Created image volume k3d-rmk-test-develop-images\nINFO[0000] Starting new tools node...\nINFO[0000] Starting node 'k3d-rmk-test-develop-tools'\nINFO[0001] Creating node 'k3d-rmk-test-develop-server-0'\nINFO[0001] Creating LoadBalancer 'k3d-rmk-test-develop-serverlb'\nINFO[0002] Pulling image 'ghcr.io/k3d-io/k3d-proxy:5.7.3'\nINFO[0015] Using the k3d-tools node to gather environment information\nINFO[0016] Starting new tools node...\nINFO[0016] Starting node 'k3d-rmk-test-develop-tools'\nINFO[0019] Starting cluster 'rmk-test-develop'\nINFO[0019] Starting servers...\nINFO[0022] Starting node 'k3d-rmk-test-develop-server-0'\nINFO[0047] All agents already running.\nINFO[0047] Starting helpers...\nINFO[0047] Starting node 'k3d-rmk-test-develop-serverlb'\nINFO[0053] Injecting records for hostAliases (incl. host.k3d.internal) and for 3 network members into CoreDNS configmap...\nINFO[0056] Cluster 'rmk-test-develop' created successfully!\nINFO[0056] You can now use it like this:\nkubectl cluster-info\n</code></pre> </li> <li> <p>Generate and encrypt secrets for    the Helmfile releases, including Nginx:</p> <pre><code>rmk secret manager generate --scope rmk-test --environment develop\nrmk secret manager encrypt --scope rmk-test --environment develop\n</code></pre> Example output <pre><code>2025-01-29T14:19:57.396+0100  INFO    generating: /home/user/rmk-test/etc/rmk-test/develop/secrets/rmk-test-app.yaml\n2025-01-29T14:19:58.993+0100  INFO    encrypting: /home/user/rmk-test/etc/rmk-test/develop/secrets/rmk-test-app.yaml\n</code></pre> </li> <li> <p>Deploy (sync) all    Helmfile releases, including Nginx, to the local K3D cluster:</p> <pre><code>rmk release sync\n</code></pre> Example output <pre><code>Release \"rmk-test-app\" does not exist. Installing it now.\nNAME: rmk-test-app\nLAST DEPLOYED: Wed Jan 29 14:23:54 2025\nNAMESPACE: rmk-test\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nThe app will be available by url:\nrmk-test-app rmk-test 1 2025-01-29 14:23:54.839083 +0100 CET deployed app-1.6.0\n</code></pre> </li> </ol> <p>At this stage, the project structure is prepared, and the application (Nginx) has been successfully deployed via Helmfile to the local K3D cluster and is now running.</p>"},{"location":"quickstart/#verifying-the-deployment","title":"Verifying the deploymentWelcome to nginx!","text":"<p>Verify the application\u2019s availability in the Kubernetes cluster:</p> <pre><code>kubectl --namespace rmk-test port-forward \"$(kubectl --namespace rmk-test get pod --output name)\" 8080:80\n</code></pre> Example output <pre><code>Forwarding from 127.0.0.1:8080 -&gt; 80\nForwarding from [::1]:8080 -&gt; 80\n</code></pre> <p>Then, open your browser and visit http://localhost:8080, or run:</p> <pre><code>open http://localhost:8080\n</code></pre> <p>You should see the Nginx welcome page.</p> Example page <p>If you see this page, the nginx web server is successfully installed and    working. Further configuration is required.</p> <p>For online documentation and support please refer to    nginx.org.    Commercial support is available at    nginx.com.</p> <p>Thank you for using nginx.</p> <p>To get list of Kubernetes pods of the <code>rmk-test</code> namespace, run:</p> <pre><code>kubectl --namespace rmk-test get pod\n</code></pre> Example output <pre><code>NAME                           READY   STATUS    RESTARTS   AGE\nrmk-test-app-bd588bfd6-ch6n7   1/1     Running   0          3s\n</code></pre>"},{"location":"quickstart/#collaborating-with-other-team-members","title":"Collaborating with other team members","text":"<p>To allow other team members to use an existing project, the initial user should commit the changes and push them to the version control system (VCS), e.g., GitHub:</p> <pre><code>git commit -am \"Generate RMK project structure, SOPS secrets, deploy Nginx release.\"\ngit push origin develop\n</code></pre> <p>After that, other team members can set up the project by pulling the latest changes:</p> <pre><code>git checkout develop\ngit pull origin develop\n</code></pre> <p>Finally, the team members should follow all the steps except the 1st one, as the project has already been generated.</p> <p>By design, SOPS Age keys (secret keys) are Git-ignored and never committed to the repository. When using a local K3D cluster, secret keys are not shared  and therefore should be recreated on another machine before proceeding with the steps:</p> <p><pre><code>rmk secret keys create\n</code></pre> Example output <pre><code>2025-01-29T16:23:00.325+0100    INFO    generate age key for scope: deps\n2025-01-29T16:23:00.326+0100    INFO    update SOPS config file: /home/user/rmk-test/etc/deps/develop/secrets/.sops.yaml\n2025-01-29T16:23:00.337+0100    INFO    generate age key for scope: rmk-test\n2025-01-29T16:23:00.338+0100    INFO    update SOPS config file: /home/user/rmk-test/etc/rmk-test/develop/secrets/.sops.yaml\n</code></pre>  If sharing the secret keys is required, consider switching from a K3D provider to a supported  cloud provider.</p>"},{"location":"release-notes/","title":"Release notes","text":"<ul> <li>Fixed kubeconfig context selection for local K3D clusters.</li> </ul>"},{"location":"configuration/cluster-management/cluster-management/","title":"Cluster management","text":""},{"location":"configuration/cluster-management/cluster-management/#overview","title":"Overview","text":"<p>RMK uses Kubernetes Cluster API and K3D for cluster management.</p> <p>RMK is suitable for both simple and complex Kubernetes deployments, enabling multi-level project inheritance through native Helmfile functionality.</p> <p>The 2 scenarios are:</p> <ul> <li>A cluster has already been provisioned using third-party tools/services: An existing Kubernetes context will be   used   by RMK.</li> <li>A cluster will be provisioned from scratch using RMK: Any of the supported cluster providers for RMK, such as   AWS,   Azure,   GCP,   K3D (local installation),   On-Premise   will be utilized.</li> </ul>"},{"location":"configuration/cluster-management/cluster-management/#switching-the-context-to-an-existing-kubernetes-cluster","title":"Switching the context to an existing Kubernetes cluster","text":"<p>Switching to an existing Kubernetes cluster depends on how it has been provisioned:</p> <ul> <li> <p>Using third-party tools/services:</p> <p>Create a context with the name strictly matching the following:</p> <pre><code>&lt;project_name&gt;-&lt;environment&gt;\n</code></pre> <p>For example, if you are in the <code>project1</code> repository in the <code>develop</code> branch, any of the following Kubernetes context will be accepted:</p> <pre><code>project1-develop\n</code></pre> </li> </ul> <ul> <li> <p>Using RMK cluster providers:</p> <p>Checkout to the branch from which the Kubernetes cluster was previously created.</p> <p>An initialization might be required, if the RMK configuration for this cluster has not been created before:</p> <pre><code>rmk config init --cluster-provider=&lt;aws|azure|gcp|k3d|onprem&gt;\n</code></pre> <p>The default value for the <code>--cluster-provider</code> argument is <code>k3d</code>.</p> <p>The next command depends on whether a remote Kubernetes cluster provider (e.g., AWS, Azure, GCP, On-Premise), or a local one (e.g., K3D) has been used:</p> <ul> <li> <p>AWS, Azure, GCP, On-Premise:</p> <pre><code># --force might be required to refresh the credentials after a long period of inactivity\nrmk cluster switch --force\n</code></pre> </li> </ul> <ul> <li> <p>K3D:</p> <p>Explicit switching to the Kubernetes context is not required, if a K3D cluster has been created already. RMK will switch implicitly, when running any of the rmk release commands.</p> </li> </ul> </li> </ul> <p>Finally, run an RMK release command to verify the preparation of the Kubernetes context, e.g.:</p> <pre><code>rmk release list\n</code></pre>"},{"location":"configuration/cluster-management/cluster-management/#using-rmk-to-prepare-capi-management-cluster","title":"Using RMK to prepare CAPI management cluster","text":"<p>Before running provisioning and destroying of cloud provider target Kubernetes clusters, a local Kubernetes Cluster API (CAPI) management cluster must be created:</p> <pre><code>rmk cluster capi create\n</code></pre> <p>Only one local CAPI management cluster can exist on the cluster administrator machine. The cluster can contain all cloud cluster providers at once and work with them independently.</p> <p>At the time of creation of the local CAPI management cluster, a Kubernetes K3D cluster with a specially lightweight configuration will be created.</p> <p>After creating the CAPI management cluster, RMK will run the clusterctl tool that initializes the installation of the cloud provider selected at the rmk config init stage. It will add specific credentials for the selected provider.</p> <p>The cloud provider version is fixed in the <code>clusterctl</code> initialization configuration file, however it can be changed on demand.</p> <p>RMK supports the following key operations for the CAPI management cluster:</p> <ul> <li> <p>Creating a CAPI management cluster based on the cloud provider initialization configuration and provided   credentials:</p> <pre><code>rmk cluster capi create \n</code></pre> </li> </ul> <ul> <li> <p>Updating   the configuration   of the installed provider, credentials, installing an additional cloud provider:</p> <pre><code>rmk cluster capi update\n</code></pre> <p>RMK allows changing the provider for the same target Kubernetes cluster simply by changing the cloud provider at the rmk config init stage and update the provider initialization configuration via rmk cluster capi update command. However, it is not recommended for <code>production</code> environments, only for <code>development</code> and <code>staging</code> or during testing.</p> </li> </ul> <ul> <li> <p>Deleting an existing CAPI management cluster:</p> <pre><code>rmk cluster capi delete\n</code></pre> <p>Important, deleting the CAPI management cluster will not delete the target Kubernetes cluster of the cloud provider.</p> </li> </ul> <p>A full list of available commands for working with CAPI management clusters and for provisioning target Kubernetes clusters can be found at the link.</p>"},{"location":"configuration/cluster-management/cluster-management/#using-rmk-cluster-providers-to-provision-and-destroy-target-kubernetes-clusters","title":"Using RMK cluster providers to provision and destroy target Kubernetes clusters","text":"<p>Currently, the following cluster providers are supported by RMK:</p> <ul> <li> <p>AWS EKS, Azure AKS, GCP GKE,    On-Premise:</p> <p>Configuration for managing remote clusters using Kubernetes Cluster API.</p> <p>Such Kubernetes clusters can be provisioned from scratch and destroyed via the rmk cluster capi provision and rmk cluster capi destroy commands. All configurations of the description of the provided target Kubernetes clusters are described in the values of the Helmfile releases, which in turn use the Helm charts we provide for each individual cloud provider.</p> <p>The Kubernetes Cluster API provider is a Kubernetes operator, meaning all configuration changes are applied declaratively. Unlike Terraform, it does not store the state of managed resources but instead uses a resource scanner to match the current configuration. If a previously created target Kubernetes cluster needs to be destroyed, but the CAPI management cluster has no record of it, you must first run rmk cluster capi provision before executing rmk cluster capi destroy.</p> </li> </ul> <ul> <li> <p>K3D:</p> <p>Configuration for managing single-machine clusters using K3D (suitable for both local development and minimal cloud deployments).</p> <p>Such Kubernetes clusters can be created from scratch and deleted via the rmk cluster k3d create and rmk cluster k3d delete commands.</p> </li> </ul> <p>When using the rmk cluster capi category commands, RMK automatically switches the Kubernetes context between the CAPI management cluster and the target Kubernetes cluster.</p> <p>On-premise support is expected in upcoming releases. This enhancement might include the introduction of additional Kubernetes Cluster API providers and Kubernetes operators.</p> <p>The main infrastructure configuration can always be checked in the cluster-deps repository.</p>"},{"location":"configuration/cluster-management/cluster-management/#using-rmk-to-update-capi-management-cluster-configuration-for-different-providers","title":"Using RMK to update CAPI management cluster configuration for different providers","text":"<p>To manage provider controller configuration, RMK uses a single CAPI management cluster on the local administrator machine.</p> <p>At the same time, RMK supports multi-tenancy for provisioning target clusters across different cloud providers: AWS EKS, Azure AKS, GCP GKE, On-Premise.</p> <p>When a CAPI management cluster is created for a specific provider, RMK performs the following initialization steps as part of the provider controller setup.</p> <p>Only one CAPI management cluster can exist on the administrator\u2019s machine. The <code>rmk cluster capi create</code> command is a singleton and can only be executed once, until the existing management cluster is deleted.</p> <p>To work with a different cloud provider in the same CAPI management cluster for another environment, you must first  initialize the RMK configuration for that environment. Then, run:</p> <pre><code>rmk cluster capi update\n</code></pre> <p>This command will initialize or update the required provider controller and install or update credentials for the selected cloud provider.</p> <p>If the required provider is not yet installed in the CAPI management cluster (as per RMK configuration initialization), you must run the <code>rmk cluster capi update</code> command before provisioning or destroying a target cluster via <code>rmk cluster capi &lt;provision|destroy&gt;</code>.</p> <p>RMK supports the following use cases for the <code>rmk cluster capi update</code> command:</p> <ul> <li>Installing or updating a provider controller.</li> <li>Creating or updating credentials for the provider controller.</li> <li> <p>Installing or updating provider extensions using Helmfile labels: <code>cluster=&lt;provider&gt;,config=extension</code>.</p> <p>For example:</p> <pre><code>  - name: aws-iam-provision-operator\n    namespace: capa-system\n    chart: \"{{`{{ .Release.Labels.repo }}`}}/app\"\n    version: \"{{`{{ .Release.Labels.appChartVersion }}`}}\"\n    labels:\n      cluster: aws\n      config: extension\n    installed: {{ eq (env \"CAPI_CLUSTER\" | default \"false\") \"true\" }}\n    inherit:\n      - template: release\n</code></pre> </li> </ul>"},{"location":"configuration/cluster-management/exported-environment-variables/","title":"Exported environment variables","text":"<p>By default, RMK exports the following environment variables to each execution of commands within the  rmk release category:</p> <pre><code># Tenant name, which is equivalent to the project name.\n# For example:\n# TENANT=rmk-test\nTENANT=\"&lt;project_name&gt;\"\n\n# RMK configuration name for the current project (tenant) and environment.\n# For example:\n# NAME=rmk-test-develop\nNAME=\"&lt;project_name&gt;-&lt;project_branch&gt;\"\n\n# Category of the executed RMK command (e.g., cluster, capi, k3d, release, secret, etc).\n# Used to enable conditional logic in various cases, such as toggling Helmfile releases.\n# For example:\n# RMK_COMMAND_CATEGORY=k3d\nRMK_COMMAND_CATEGORY=\"&lt;command_category&gt;\"\n\n# Root domain name for target applications in the cluster.\n# For example:\n# ROOT_DOMAIN=rmk-test-develop.edenlab.dev\nROOT_DOMAIN=\"&lt;project_name&gt;-&lt;project_branch&gt;.&lt;domain&gt;\"\n\n# Path to the file containing the merged private SOPS Age keys.\n# For example:\n# SOPS_AGE_KEY_FILE=/home/user/.rmk/sops-age-keys/rmk-test/.keys.txt\nSOPS_AGE_KEY_FILE=\"${HOME}/.rmk/sops-age-keys/&lt;project_name&gt;/.keys.txt\"\n\n# GitHub Personal Access Token (PAT). \n# https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens\n# For example:\n# GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nGITHUB_TOKEN=\"&lt;github_personal_access_token&gt;\"\n\n# Path to the directory containing the specific version of hooks required by the downstream (current) project.\n# For example:\n# HELMFILE_RMK_TEST_HOOKS_DIR=/home/user/rmk-test/.PROJECT/inventory/hooks/helmfile.hooks.infra-v1.29.1\nHELMFILE_&lt;project_name&gt;_HOOKS_DIR=\"${PWD}/.PROJECT/inventory/hooks/&lt;project_hooks_repo_name&gt;-&lt;project_hooks_version&gt;\"\n\n# Paths to the directories of the inherited upstream projects required by the downstream (current) project (in JSON format).\n# For example:\n# HELMFILE_RMK_TEST_PATHS=[{\"path\":\"/home/user/rmk-test/.PROJECT/dependencies/rmk-test.bootstrap.infra-v0.1.0/helmfile.yaml.gotmpl\"}]\nHELMFILE_&lt;project_name&gt;_PATHS='[{\"path\":\"${PWD}/.PROJECT/dependencies/&lt;upstream_project_name&gt;-&lt;upstream_project_version&gt;/helmfile.yaml.gotmpl\"}]'\n\n# Path to the directory containing the specific version of hooks required by the inherited upstream project.\n# For example:\n# HELMFILE_CLUSTER_DEPS_HOOKS_DIR=/home/user/rmk-test/.PROJECT/inventory/hooks/helmfile.hooks.infra-v1.29.1\nHELMFILE_&lt;upstream_project_name&gt;_HOOKS_DIR=\"${PWD}/.PROJECT/inventory/hooks/&lt;upstream_project_hooks_repo_name&gt;-&lt;upstream_project_hooks_version&gt;\"\n\n# Version of the inherited upstream project.\n# For example:\n# HELMFILE_CLUSTER_DEPS_BOOTSTRAP_INFRA_VERSION=v0.1.0\nHELMFILE_&lt;upstream_project_name&gt;_VERSION=\"&lt;upstream_project_version&gt;\"\n\n# Flag indicating whether the cluster is K3D (Kubernetes Cluster API management cluster).\n# The variable can be used to override values in the releases.\nCAPI_CLUSTER=false\n# The flag that indicates whether the cluster is K3D (local cluster). \n# The variable can be used to override values in the releases.\nK3D_CLUSTER=false\n# The K3D cluster name equaling exported environment variable $NAME.\nK3D_NAME=\"${NAME}\"\n# The local directory path for mount into K3D cluster.\n# By default, the RMK startup path is used.\nK3D_VOLUME_HOST_PATH=\"${PWD}\"\n\n# Only for Kubernetes Cluster API Provider AWS: \n# AWS cluster provider configuration\n# Note: An AWS access key must be created in advance:\n#       https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html\n#       AWS config and credentials files will be automatically generated by RMK during the configuration initialization\n# For example:\n# AWS_ACCOUNT_ID=123456789012\n# AWS_REGION=us-east-1\n# AWS_PROFILE=rmk-test-develop\n# AWS_CONFIG_FILE=/home/user/.aws/config_rmk-test-develop\n# AWS_SHARED_CREDENTIALS_FILE=/home/user/.aws/credentials_rmk-test-develop\nAWS_ACCOUNT_ID=\"&lt;aws_account_id&gt;\"\nAWS_CLUSTER=true\nAWS_REGION=\"&lt;aws_region&gt;\"\nAWS_PROFILE=\"&lt;project_name&gt;-&lt;project_branch&gt;\"\nAWS_CONFIG_FILE=\"${HOME}/.aws/config_&lt;project_name&gt;-&lt;project_branch&gt;\"\nAWS_SHARED_CREDENTIALS_FILE=\"${HOME}/.aws/credentials_&lt;project_name&gt;-&lt;project_branch&gt;\"\n\n# Only for Kubernetes Cluster API Provider Azure:\n# Azure cluster provider configuration\n# Note: Azure service principal must be created in advance:\n#       https://learn.microsoft.com/en-us/entra/identity-platform/app-objects-and-service-principals\n# For example:\n# AZURE_LOCATION=eastus\n# AZURE_SUBSCRIPTION_ID=abcdef12-3456-7890-abcd-ef1234567890\nAZURE_CLUSTER=true\nAZURE_LOCATION=\"&lt;azure_location&gt;\"\nAZURE_SUBSCRIPTION_ID=\"&lt;azure_subscription_id&gt;\"\n\n# Only for Kubernetes Cluster API Provider GCP:\n# Google Cloud cluster provider configuration\n# Note: Google Cloud application credentials must be created in advance: \n#       https://cloud.google.com/docs/authentication/application-default-credentials \n# For example:\n# GCP_PROJECT_ID=rmk-test\n# GCP_REGION=us-east1\n# GOOGLE_APPLICATION_CREDENTIALS=/home/user/.config/gcloud/gcp-credentials-rmk-test-develop.json\nGCP_CLUSTER=true\nGCP_PROJECT_ID=\"&lt;google_cloud_project_id&gt;\"\nGCP_REGION=\"&lt;google_cloud_region&gt;\"\nGOOGLE_APPLICATION_CREDENTIALS=\"${HOME}/.config/gcloud/gcp-credentials-&lt;project_name&gt;-&lt;project_branch&gt;.json\"\n\n# Only for Kubernetes Cluster API Provider On-Premise:\n# On-Premise configuration\n# For example:\n# ONPREM_CLUSTER=false\nONPREM_CLUSTER=true\n</code></pre> <p>To view the complete list of all possible exported variables and their values for the rmk release command category, run the following command in <code>debug</code> mode:</p> <pre><code>rmk --log-level=debug release list --skip-context-switch\n</code></pre>"},{"location":"configuration/cluster-management/usage-aws-provider/","title":"Using AWS cluster provider","text":"<p>AWS users must have the PowerUserAccess, SecretsManagerReadWrite permissions to be able to provision and destroy AWS EKS clusters.</p> <p>Before provisioning the Kubernetes cluster, add override for the configuration file to the <code>deps</code> scope for the target Kubernetes cluster.</p> <pre><code># A complete list of all options can be found here: https://capz.sigs.k8s.io/reference/v1beta1-api\ncontrolPlane:\n  spec:\n    iamAuthenticatorConfig:\n      # UserMappings is a list of user mappings\n      mapUsers:\n        # TODO: Add a list of users at the downstream project repository level\n        - groups:\n            - system:masters\n          # UserARN is the AWS ARN for the user to map\n          userarn: arn:aws:iam::{{ env \"AWS_ACCOUNT_ID\" }}:user/user1\n          # UserName is a kubernetes RBAC user subject*/}}\n          username: user1\n    version: v1.29.8 # ^v?(0|[1-9][0-9]*)\\.(0|[1-9][0-9]*)\\.?(\\.0|[1-9][0-9]*)?$\n\n## The machine pools configurations\nmachinePools:\n  app:\n    enabled: true\n    managed:\n      spec:\n        instanceType: t3.medium\n        # Labels specifies labels for the Kubernetes node objects\n        labels:\n          db: app\n        # Scaling specifies scaling for the ASG behind this pool\n        scaling:\n          maxSize: 1\n          minSize: 1\n    # Number of desired machines. Defaults to 1.\n    replicas: 1\n# ...\n</code></pre> <p>Using the example above and the example from the cluster-deps repository you can add the required number of machine pools depending on the requirements for distribution into individual roles.</p> <p>For the AWS provider, before launching the actual provisioning of the cluster, RMK will perform the following preliminary steps:</p> <ul> <li>Create an SSH key pair for cluster nodes.</li> <li>Create secrets with private SOPS Age keys in the   AWS Secret Manager, if they have not been created previously.</li> </ul> <p>To start provisioning a Kubernetes cluster, run the commands:</p> <pre><code>rmk cluster capi provision\n</code></pre> <p>When the cluster is ready, RMK automatically switches the Kubernetes context to the newly created cluster.</p> <p>To destroy a Kubernetes cluster, run the command:</p> <pre><code>rmk cluster capi destroy\n</code></pre> <p>After the cluster is destroyed, RMK will delete the previously created SSH key and the context for the target Kubernetes cluster.</p>"},{"location":"configuration/cluster-management/usage-azure-provider/","title":"Using Azure cluster provider","text":"<p>Azure service principal must have the Contributor, Key Vault Secrets Officer roles to be able to provision and destroy Azure AKS clusters.</p> <p>Before provisioning the Kubernetes cluster, add override for the configuration file to the <code>deps</code> scope for the target Kubernetes cluster.</p> <pre><code>controlPlane:\n  spec:\n    ## Kubernetes version\n    version: v1.29.8\n\nmachinePools:\n  system:\n    enabled: true\n\n  app:\n    enabled: true\n    replicas: 1\n    spec:\n      mode: User\n      sku: Standard_B2ls_v2\n      osDiskSizeGB: 30\n      nodeLabels:\n        db: app\n      scaling:\n        minSize: 1\n        maxSize: 1\n# ...\n</code></pre> <p>Using the example above and the example from the cluster-deps repository you can add the required number of machine pools depending on the requirements for distribution into individual roles.</p> <p>For the Azure provider, before launching the actual provisioning of the cluster, RMK will perform the following preliminary steps:</p> <ul> <li>Create secrets with private SOPS Age keys in the   Azure Key Vault, if they have not been created previously.</li> </ul> <p>To start provisioning a Kubernetes cluster, run the commands:</p> <pre><code>rmk cluster capi provision\n</code></pre> <p>When the cluster is ready, RMK automatically switches the Kubernetes context to the newly created cluster.</p> <p>To destroy a Kubernetes cluster, run the command:</p> <pre><code>rmk cluster capi destroy\n</code></pre> <p>After the cluster is destroyed, RMK will delete the context for the target Kubernetes cluster.</p>"},{"location":"configuration/cluster-management/usage-gcp-provider/","title":"Using GCP cluster provider","text":"<p>GCP service account must have the <code>Editor</code>, <code>Secret Manager Admin</code>, <code>Kubernetes Engine Admin</code> roles to be able to provision and destroy GCP GKE clusters.</p> <p>Before provisioning the Kubernetes cluster, add override the configuration file to the <code>deps</code> scope for the target Kubernetes cluster.</p> <pre><code>controlPlane:\n  spec:\n    version: \"v1.30.5\"\n\nmachinePools:\n  app:\n    enabled: true\n    managed:\n      spec:\n        # MachineType is the name of a Google Compute Engine\n        # (https://cloud.google.com/compute/docs/machine-types).\n        # If unspecified, the default machine type is `e2-medium`.\n        machineType: \"e2-medium\"\n        management:\n          # AutoUpgrade specifies whether node auto-upgrade is enabled for the node\n          # pool. If enabled, node auto-upgrade helps keep the nodes in your node pool\n          # up to date with the latest release version of Kubernetes.\n          autoUpgrade: true\n        # MaxPodsPerNode is constraint enforced on the max num of pods per node.\n    replicas: 1\n# ...\n</code></pre> <p>Using the example above and the example from the cluster-deps repository you can add the required number of machine pools depending on the requirements for distribution into individual roles.</p> <p>For the GCP provider, before launching the actual provisioning of the cluster, RMK will perform the following preliminary steps:</p> <ul> <li>Create a Cloud NAT for outbound traffic cluster nodes.</li> <li>Create secrets with private SOPS Age keys in the   GCP Secret Manager,    if they have not been created previously.</li> </ul> <p>To start provisioning a Kubernetes cluster, run the commands:</p> <pre><code>rmk cluster capi provision\n</code></pre> <p>When the cluster is ready, RMK automatically switches the Kubernetes context to the newly created cluster.</p> <p>To destroy a Kubernetes cluster, run the command:</p> <pre><code>rmk cluster capi destroy\n</code></pre> <p>After the cluster is destroyed, RMK will delete the previously created Cloud NAT (if this resource is no longer used by other clusters in the same region) together with the context for the target Kubernetes cluster.</p>"},{"location":"configuration/cluster-management/usage-k3d-provider/","title":"Using K3D cluster provider","text":"<p>RMK supports managing single-node Kubernetes clusters using K3D.</p> <p>The CLI will create a cluster according to the declarative configuration for K3D.</p>"},{"location":"configuration/cluster-management/usage-k3d-provider/#creating-k3d-clusters","title":"Creating K3D clusters","text":"<p>Run the following command:</p> <pre><code>rmk cluster k3d create\n</code></pre> <p>By default, RMK will use the current directory for the \u2013k3d-volume-host-path flag.</p> <p>When the Kubernetes cluster is ready, RMK automatically switches the Kubernetes context to the newly created cluster. You can create multiple local K3D clusters by separating them using Git branches.</p>"},{"location":"configuration/cluster-management/usage-k3d-provider/#deleting-k3d-clusters","title":"Deleting K3D clusters","text":"<p>Run the following command:</p> <pre><code>rmk cluster k3d delete\n</code></pre>"},{"location":"configuration/cluster-management/usage-onprem-provider/","title":"Using On-Premise cluster provider","text":"<p>Ensure that all target machines are accessible over the network via SSH and properly configured for K3S installation.</p> <p>Before provisioning the Kubernetes cluster, add override for the configuration file to the <code>deps</code> scope for the target Kubernetes cluster.</p> <pre><code># A complete list of all options can be found here: https://github.com/edenlabllc/on-premise-configurator.operators.infra/blob/develop/watches.yaml\ntemplates:\n  machineSpec: &amp;machineSpec\n    bootstrap:\n      # DataSecretName is the name of the secret that stores the bootstrap data script.\n      # If nil, the Machine should remain in the Pending state.\n      dataSecretName: \"\"\n\n  machineRemoteServerSpec: &amp;machineRemoteServerSpec\n    # providerID will automatically be set by edenlabllc/on-premise-configurator.operators.infra\n    k3sAirGapEnabled: false\n    k3sServerConfigYAML: |\n      node-taint:\n        - node-role.kubernetes.io/control-plane=:NoSchedule\n      tls-san:\n        - 192.0.2.10\n        - 192.0.2.11\n        - 192.0.2.12\n    k3sRole: server\n    sshUser: user\n\n  machineRemoteAgentSpec: &amp;machineRemoteAgentSpec\n    # providerID will automatically be set by edenlabllc/on-premise-configurator.operators.infra\n    k3sAirGapEnabled: false\n    k3sRole: agent\n    sshUser: user\n\n  controlPlaneHostInternal: &amp;controlPlaneHostInternal \"192.0.2.10\" # e.g. 192.0.2.10\n\n# A minimum of 3 control plane machines is required, when k3sHAMode=true\ncontrolPlane:\n  endpoint:\n    host: *controlPlaneHostInternal\n    port: 6443\n\n  spec:\n    k3sHAMode: true\n    ## Kubernetes version\n    version: v1.31.9+k3s1\n\n  ## The control plane machines (k3sRole=server)\n  machines:\n    k3s-control-plane-0:\n      enabled: true\n      remote:\n        spec:\n          address: *controlPlaneHostInternal\n          k3sInitServer: true\n          &lt;&lt;: *machineRemoteServerSpec\n      spec:\n        &lt;&lt;: *machineSpec\n\n    k3s-control-plane-1:\n      enabled: true\n      remote:\n        spec:\n          address: 192.0.2.11\n          k3sApiEndpoint: *controlPlaneHostInternal\n          k3sInitServer: false\n          &lt;&lt;: *machineRemoteServerSpec\n      spec:\n        &lt;&lt;: *machineSpec\n\n    k3s-control-plane-2:\n      enabled: true\n      remote:\n        spec:\n          address: 192.0.2.12\n          k3sApiEndpoint: *controlPlaneHostInternal\n          k3sInitServer: false\n          &lt;&lt;: *machineRemoteServerSpec\n      spec:\n        &lt;&lt;: *machineSpec\n\n## The worker machines (k3sRole=agent)\nmachines:\n  ## Example load balancer machine\n  load-balancer:\n    enabled: true\n    remote:\n      spec:\n        address: 192.0.2.100\n        k3sAgentConfigYAML: |\n          node-label:\n            - svccontroller.k3s.cattle.io/lbpool=pool1\n            - svccontroller.k3s.cattle.io/enablelb=true\n        k3sApiEndpoint: *controlPlaneHostInternal\n        &lt;&lt;: *machineRemoteAgentSpec\n    spec:\n      &lt;&lt;: *machineSpec\n\n  ## Example stateless app machine\n  app-stateless:\n    enabled: true\n    remote:\n      spec:\n        address: 192.0.2.101\n        # agent config is optional\n        # k3sAgentConfigYAML: |\n        k3sApiEndpoint: *controlPlaneHostInternal\n        &lt;&lt;: *machineRemoteAgentSpec\n    spec:\n      &lt;&lt;: *machineSpec\n\n  ## Example stateful app machine\n  app-stateful:\n    enabled: true\n    remote:\n      spec:\n        address: 192.0.2.102\n        k3sAgentConfigYAML: |\n          node-label:\n            - db=mydb\n          node-taint:\n            - key=mydb:NoSchedule\n        k3sApiEndpoint: *controlPlaneHostInternal\n        &lt;&lt;: *machineRemoteAgentSpec\n    spec:\n      &lt;&lt;: *machineSpec\n</code></pre> <p>Using the example above and the example from the cluster-deps repository you can add the required number of machines depending on the requirements.</p> <p>For the On-Premise provider, before launching the actual provisioning of the cluster, RMK will create a K8S secret <code>capop-ssh-identity-secret</code> in the <code>capop-system</code> namespace of the CAPI Management cluster storing the provided SSH private key.</p> <p>Only one SSH identity secret can be active at a time, which means that only a single target on-premise cluster can be managed concurrently. To switch to another cluster, reconfigure the provider with the corresponding SSH key, which will replace the existing secret, then update the CAPI Management cluster to apply the changes.</p> <p>To start provisioning a Kubernetes cluster, run the commands:</p> <pre><code>rmk cluster capi provision\n</code></pre> <p>When the cluster is ready, RMK automatically switches the Kubernetes context to the newly created cluster.</p> <p>To destroy a Kubernetes cluster, run the command:</p> <pre><code>rmk cluster capi destroy\n</code></pre> <p>After the cluster is destroyed, RMK will delete the previously created K8S secret with the SSH private key  as well as the K8S context for the target Kubernetes cluster.</p>"},{"location":"configuration/configuration-management/configuration-management/","title":"Configuration management","text":""},{"location":"configuration/configuration-management/configuration-management/#overview","title":"Overview","text":"<p>To start working with Kubernetes clusters, RMK needs to initialize the configuration for the current environment.</p> <p>At the time of configuration initialization launch, RMK prepares the state in the form of the current environment config with all the required attributes for further work. It also downloads and resolves and installs all necessary dependencies and tools described in the project.yaml file in the root of the project repository.</p>"},{"location":"configuration/configuration-management/configuration-management/#list-of-main-attributes-of-the-rmk-configuration","title":"List of main attributes of the RMK configuration","text":"<p>Example of the configuration per cluster provider:</p> <ul> <li>AWS</li> <li>Azure</li> <li>GCP</li> <li>K3D</li> <li>On-Premise</li> </ul> <p>All configuration attributes can be overridden using RMK flags or environment variables.</p> <p>To view the available options of the created configuration, use the command:</p> <pre><code>rmk config view\n</code></pre>"},{"location":"configuration/configuration-management/configuration-management/#understanding-the-behavior-of-the-configuration-initialization-command","title":"Understanding the behavior of the configuration initialization command","text":"<p>The rmk config init command supports declarative behavior within a single project repository and an environment that equal branch name.</p> <p>This example assumes the project (tenant) name is <code>rmk-test</code>, the Git branch and environment are <code>develop</code>, the generated RMK configuration name is <code>rmk-test-develop</code>, the selected cluster provider is <code>aws</code>:</p> <pre><code>rmk config init --cluster-provider=aws \\ \n  --github-token=&lt;github_personal_access_token&gt; \\\n  --aws-access-key-id=&lt;aws_access_key_id&gt; \\\n  --aws-region=us-east-1 \\\n  --aws-secret-access-key=&lt;aws_secret_access_key&gt;\n</code></pre> <p>In the example above, this configuration was applied for the first time, setting the options accordingly. After that, there\u2019s no need to re-specify the entire list of values - simply update the required option as needed.</p> <p>For example:</p> <pre><code># no need to specify the AWS flags again \nrmk config init --github-token=&lt;new_github_personal_access_token&gt;\n</code></pre>"},{"location":"configuration/configuration-management/configuration-management/#initialization-of-rmk-configuration","title":"Initialization of RMK configuration","text":""},{"location":"configuration/configuration-management/configuration-management/#prerequisites","title":"Prerequisites","text":"<ul> <li>Project repository has already been created and   initialized.</li> <li>At least one Git branch for the environment exists already.</li> </ul>"},{"location":"configuration/configuration-management/configuration-management/#command","title":"Command","text":"<pre><code>rmk config init\n</code></pre>"},{"location":"configuration/configuration-management/configuration-management/#initialization-of-rmk-configuration-for-private-github-repositories","title":"Initialization of RMK configuration for private GitHub repositories","text":""},{"location":"configuration/configuration-management/configuration-management/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>The <code>GITHUB_TOKEN</code> variable or <code>--github-token</code> flag are required:   GitHub Personal Access Tokens (PAT).</li> <li>The token should have the <code>repo: full control</code> permissions.</li> </ul>"},{"location":"configuration/configuration-management/configuration-management/#command_1","title":"Command","text":"<pre><code>rmk config init --github-token=&lt;github_personal_access_token&gt;\n</code></pre>"},{"location":"configuration/configuration-management/configuration-management/#initialization-of-rmk-configuration-for-different-cluster-providers","title":"Initialization of RMK configuration for different cluster providers","text":"<ul> <li>AWS</li> <li>Azure</li> <li>GCP</li> <li>K3D</li> <li>On-Premise</li> </ul>"},{"location":"configuration/configuration-management/configuration-management/#initialization-of-rmk-configuration-with-a-custom-root-domain","title":"Initialization of RMK configuration with a custom root domain","text":"<p>To change the root domain name, you need to edit the project.yaml file in the section <code>develop.root-domain</code>.</p> <pre><code>project:\n  spec:\n    environments:\n      develop:\n        root-domain: test.example.com\n</code></pre> <p>Then run the following command:</p> <pre><code>rmk config init\n</code></pre>"},{"location":"configuration/configuration-management/configuration-management/#deletion-of-rmk-configuration","title":"Deletion of RMK configuration","text":"<pre><code>rmk config delete\n</code></pre> <p>When deleting the current RMK configuration, the respective cluster providers files will be deleted as well.</p>"},{"location":"configuration/configuration-management/init-aws-provider/","title":"Initialization of AWS cluster provider","text":""},{"location":"configuration/configuration-management/init-aws-provider/#list-of-main-attributes-of-the-rmk-configuration","title":"List of main attributes of the RMK configuration","text":""},{"location":"configuration/configuration-management/init-aws-provider/#with-mfa","title":"With MFA","text":"<pre><code>name: rmk-test-develop\ntenant: rmk-test\nenvironment: develop\nroot-domain: rmk-test-develop.edenlab.dev\ncluster-provider: aws\n# ...\naws-mfa-profile: rmk-test-develop-mfa # AWS profile name for MFA.\naws-mfa-token-expiration: \"1738006158\" # Time expiration MFA token.\naws:\n  account_id: \"123456789012\"\n  config-source: /home/user/.aws/config_rmk-test-develop\n  credentials-source: /home/user/.aws/credentials_rmk-test-develop\n  user-name: user\n  mfa-device: arn:aws:iam::123456789012:mfa/user # MFA device AWS ARN.\n  profile: rmk-test-develop\n  region: us-east-1\n</code></pre>"},{"location":"configuration/configuration-management/init-aws-provider/#without-mfa","title":"Without MFA","text":"<pre><code>name: rmk-test-develop # RMK config name, a unique identifier which consists of the project (tenant) name and the abbreviated name of the Git branch.\ntenant: rmk-test # Tenant name, which is equivalent to the project name.\nenvironment: develop # Environment name.\nroot-domain: rmk-test-develop.edenlab.dev # Root domain name used across the cluster.\ncluster-provider: aws # Selected cluster provider.\n# ...\naws:\n  account_id: \"123456789012\" # AWS account ID.\n  config-source: /home/user/.aws/config_rmk-test-develop # Absolute path to the AWS profile config.\n  credentials-source: /home/user/.aws/credentials_rmk-test-develop # Absolute path to the AWS profile credentials.\n  user-name: user # AWS user.\n  profile: rmk-test-develop # AWS profile name.\n  region: us-east-1 # AWS region of the current Kubernetes cluster.    \n# ...\n</code></pre>"},{"location":"configuration/configuration-management/init-aws-provider/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Having an AWS account and a created user with access policies in IAM:    PowerUserAccess,    SecretsManagerReadWrite.</p> <p>See the useful link.</p> </li> <li> <p>Having an AWS access key pair.</p> <p>See the useful link.</p> </li> <li> <p>Allocated EC2 quotas for specific    family VMs in the required region.</p> </li> </ol>"},{"location":"configuration/configuration-management/init-aws-provider/#configuration","title":"Configuration","text":"<p>If an AWS profile with the correct name was not created during the initial configuration, RMK will generate it and store the AWS config and credentials files at the following path:</p> <pre><code>${HOME}/.aws/config_&lt;project_name&gt;-&lt;project_branch&gt;\n${HOME}/.aws/credentials_&lt;project_name&gt;-&lt;project_branch&gt;\n</code></pre> <p>The 2 supported configuration scenarios are:</p> <ul> <li>using RMK flags:   <pre><code>rmk config init --cluster-provider=aws \\\n  --aws-access-key-id=&lt;aws_access_key_id&gt; \\\n  --aws-region=us-east-1 \\\n  --aws-secret-access-key=&lt;aws_secret_access_key&gt;\n</code></pre></li> </ul> <ul> <li>using environment variables: <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_REGION</code>, <code>AWS_SECRET_ACCESS_KEY</code>.   <pre><code>export AWS_ACCESS_KEY_ID=&lt;aws_access_key_id&gt;\nexport AWS_REGION=us-east-1\nexport AWS_SECRET_ACCESS_KEY=&lt;aws_secret_access_key&gt;\nrmk config init --cluster-provider=aws\n</code></pre></li> </ul> <p>If environment variables were set before running the command, RMK will create a profile based on their values. If flags are specified, RMK will prioritize them over environment variables, as CLI flags take precedence.</p>"},{"location":"configuration/configuration-management/init-aws-provider/#support-for-multi-factor-authentication-mfa","title":"Support for multi-factor authentication (MFA)","text":"<p>RMK automatically check for an MFA device, when the following command is executed:</p> <pre><code>rmk config init --cluster-provider=aws\n</code></pre> <p>To set up an MFA device, if it is required by the administrator, the following actions should be executed:</p> <ol> <li>Sign in to the AWS Management Console.</li> <li>Go to the following page to set up security    credentials: My security credentials</li> <li>Navigate to the \u201cMulti-factor authentication (MFA)\u201d section and set up an MFA device.    If a device name is required, specify a name.</li> <li>After that, sign out and sign in again to refresh AWS policies    (might be required in case of an IAM policy based on the    aws:MultiFactorAuthPresent    condition exists).</li> <li>Finally, on the \u201cMy security credentials\u201d page navigate to the \u201cAccess keys\u201d section    and create a new AWS access key, if needed.</li> </ol> <p>For the detailed documentation regarding the MFA setup in AWS, go to AWS documentation.</p> <p>You can also check the lifetime of the session token by running the rmk config init command:</p> <pre><code>2022-12-14T09:02:20.267+0100 INFO MFA remaining time for token validity: 11:59:48\n</code></pre>"},{"location":"configuration/configuration-management/init-aws-provider/#reconfiguration-of-the-aws-profile-if-wrong-credentials-has-been-input","title":"Reconfiguration of the AWS profile if wrong credentials has been input","text":"<p>Modify the value of a specific flag if changes are needed:</p> <pre><code>rmk config init --aws-access-key-id=&lt;new_aws_access_key_id&gt; --aws-secret-access-key=&lt;new_aws_secret_access_key&gt;\n</code></pre>"},{"location":"configuration/configuration-management/init-azure-provider/","title":"Initialization of Azure cluster provider","text":""},{"location":"configuration/configuration-management/init-azure-provider/#list-of-main-attributes-of-the-rmk-configuration","title":"List of main attributes of the RMK configuration","text":"<pre><code>name: rmk-test-develop # RMK config name, a unique identifier which consists of the project (tenant) name and the abbreviated name of the Git branch.\ntenant: rmk-test # Tenant name, which is equivalent to the project name.\nenvironment: develop # Environment name.\nroot-domain: rmk-test-develop.edenlab.dev # Root domain name used across the cluster.\ncluster-provider: azure # Selected cluster provider.\n# ...\nazure:\n  key-vault:\n    key-vault-name: kv-ecc1c839a7b9bf5e # Azure Key Vault autogenerate name.\n    key-vault-uri: https://kv-ecc1c839a7b9bf5e.vault.azure.net/ # Azure Key Vault API URL.\n    resource-group-name: rmk-test-sops-age-keys # Azure resource group name for Key Vault.\n  location: eastus # Azure location of the current Kubernetes cluster.\n  subscription-id: abcdef12-3456-7890-abcd-ef1234567890 # Azure subscription ID.\n# ...\n</code></pre>"},{"location":"configuration/configuration-management/init-azure-provider/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Having an Azure subscription and a created service principal with access roles in IAM:    Contributor,    Key Vault Secrets Officer.</p> <p>See the useful link.</p> </li> <li> <p>Enable the following resource    providers: <code>Microsoft.Authorization</code>, <code>Microsoft.Compute</code>, <code>Microsoft.ContainerService</code>, <code>Microsoft.ManagedIdentity</code>, <code>Microsoft.Network</code>.</p> <p>See the useful link.</p> </li> <li> <p>Allocated quotas for specific family VMs in the    required region.</p> </li> </ol>"},{"location":"configuration/configuration-management/init-azure-provider/#configuration","title":"Configuration","text":"<p>If an Azure service principal file was not created during the initial configuration, RMK will generate it automatically and store it at the following path:</p> <pre><code>${HOME}/.azure/service-principal-credentials_&lt;project_name&gt;-&lt;project_branch&gt;.json\n</code></pre> <p>The 3 supported configuration scenarios are:</p> <ul> <li>via RMK flags:   <pre><code>rmk config init --cluster-provider=azure \\ \n  --azure-client-id=&lt;azure_client_id&gt; \\\n  --azure-client-secret=&lt;azure_client_secret&gt; \\\n  --azure-location=eastus \\\n  --azure-subscription-id=&lt;azure_subscription_id&gt; \\ \n  --azure-tenant-id=&lt;azure_tenant_id&gt;\n</code></pre></li> </ul> <ul> <li>via environment variables   : <code>AZURE_CLIENT_ID</code>, <code>AZURE_CLIENT_SECRET</code>, <code>AZURE_LOCATION</code>, <code>AZURE_SUBSCRIPTION_ID</code>, <code>AZURE_TENANT_ID</code>.   <pre><code>export AZURE_CLIENT_ID=&lt;azure_client_id&gt;\nexport AZURE_CLIENT_SECRET=&lt;azure_client_secret&gt;\nexport AZURE_LOCATION=eastus\nexport AZURE_SUBSCRIPTION_ID=&lt;azure_subscription_id&gt;\nexport AZURE_TENANT_ID=&lt;azure_tenant_id&gt;\nrmk config init --cluster-provider=azure\n</code></pre></li> </ul> <ul> <li>via <code>STDIN</code> using output of the az CLI:   <pre><code># login interactively: https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli-interactively#interactive-login\naz login\naz ad sp create-for-rbac --name rmk-test --role contributor --scopes=\"/subscriptions/&lt;azure_subscription_id&gt;\" --output json | \\\n  rmk config init --cluster-provider=azure --azure-location=eastus --azure-service-principle\n</code></pre></li> </ul> <p>If environment variables were set before running the command, RMK will create an Azure service principal file based on their values. If flags are specified, RMK will prioritize them over environment variables, as CLI flags take precedence.</p>"},{"location":"configuration/configuration-management/init-azure-provider/#custom-resource-group-name-for-azure-key-vault","title":"Custom resource group name for Azure Key Vault","text":"<p>By default, RMK generates the following Azure Key Vault  resource group name:</p> <pre><code>&lt;project_name&gt;-sops-age-keys\n</code></pre> <p>For example:</p> <pre><code>rmk-test-sops-age-keys\n</code></pre> <p>If the service principal used for authorization has the  Key Vault Secrets Officer role with a scope, e.g.:</p> <pre><code>/subscriptions/&lt;subscription-id&gt;/resourceGroups/&lt;resource_group&gt;\n</code></pre> <p>pointing to an Azure Key Vault resource group, RMK will automatically determine its name based on this scope.</p> <p>Alternatively, you can manually specify the custom resource group name when initializing RMK configuration for the  current environment using the optional <code>--azure-key-vault-resource-group-name=&lt;key_vault_resource_group&gt;</code> flag:</p> <pre><code>rmk config init --cluster-provider=azure \\ \n    --azure-client-id=&lt;azure_client_id&gt; \\\n    --azure-client-secret=&lt;azure_client_secret&gt; \\\n    --azure-location=eastus \\\n    --azure-subscription-id=&lt;azure_subscription_id&gt; \\ \n    --azure-tenant-id=&lt;azure_tenant_id&gt; \\\n    --azure-key-vault-resource-group-name=&lt;key_vault_resource_group&gt; # Optional\n</code></pre>"},{"location":"configuration/configuration-management/init-azure-provider/#reconfiguration-of-the-azure-service-principal-attributes-if-wrong-credentials-has-been-input","title":"Reconfiguration of the Azure service principal attributes if wrong credentials has been input","text":"<p>Modify the value of a specific flag if changes are needed:</p> <pre><code>rmk config init --azure-client-id=&lt;new_azure_client_id&gt; --azure-client-secret=&lt;new_azure_client_secret&gt;\n</code></pre>"},{"location":"configuration/configuration-management/init-gcp-provider/","title":"Initialization of GCP cluster provider","text":""},{"location":"configuration/configuration-management/init-gcp-provider/#list-of-main-attributes-of-the-rmk-configuration","title":"List of main attributes of the RMK configuration","text":"<pre><code>name: rmk-test-develop # RMK config name, a unique identifier which consists of the project (tenant) name and the abbreviated name of the Git branch.\ntenant: rmk-test # Tenant name, which is equivalent to the project name.\nenvironment: develop # Environment name.\nroot-domain: rmk-test-develop.edenlab.dev # Root domain name used across the cluster.\ncluster-provider: gcp # Selected cluster provider.\n# ...\ngcp-region: us-east1 # GCP region of the current Kubernetes cluster.\ngcp:\n  app-credentials-path: /home/user/.config/gcloud/gcp-credentials-rmk-test-develop.json # Absolute path to GCP service account file.   \n  project-id: project-name # GCP project name. Got from GCP service account file.\n# ...\n</code></pre>"},{"location":"configuration/configuration-management/init-gcp-provider/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Having a GCP account and project and a created service account with access roles in IAM: <code>Editor</code>, <code>Secret    Manager Admin</code>, <code>Kubernetes Engine Admin</code>.</p> <p>See the useful link.</p> </li> <li> <p>Enable the following services    API: <code>Kubernetes Engine API (container.googleapis.com)</code>, <code>Cloud Compute Engine API (compute.googleapis.com)</code>, <code>Cloud NAT API (servicenetworking.googleapis.com)</code>, <code>IAM Service API (iam.googleapis.com)</code>, <code>Cloud Logging API (logging.googleapis.com)</code>, <code>Cloud Monitoring API (monitoring.googleapis.com)</code>.</p> <p>See the useful link.</p> </li> <li> <p>Allocated quotas for specific family VMs in the required region.</p> </li> </ol>"},{"location":"configuration/configuration-management/init-gcp-provider/#configuration","title":"Configuration","text":"<p>If a GCP service account key file with the correct name was not created during the initial configuration, RMK will generate it automatically and store it at the following path:</p> <pre><code>${HOME}/.config/gcloud/gcp-credentials-&lt;project_name&gt;-&lt;project_branch&gt;.json\n</code></pre> <p>The 2 supported configuration scenarios are:</p> <ul> <li>via RMK flags:   <pre><code>rmk config init --cluster-provider=gcp \\\n  --gcp-region=us-east1 \\\n  --google-application-credentials &lt;path_to_exported_google_service_account_file&gt;\n</code></pre></li> </ul> <ul> <li>via environment variables: <code>GCP_REGION</code>, <code>GOOGLE_APPLICATION_CREDENTIALS</code>.   <pre><code>export GCP_REGION=us-east1\nexport GOOGLE_APPLICATION_CREDENTIALS=&lt;path_to_exported_google_service_account_file&gt;\nrmk config init --cluster-provider=gcp\n</code></pre></li> </ul> <p>If environment variables were set before running the command, RMK will create a GCP service account file based on their values. If flags are specified, RMK will prioritize them over environment variables, as CLI flags take precedence.</p>"},{"location":"configuration/configuration-management/init-gcp-provider/#reconfiguration-of-the-gcp-service-account-attributes-if-wrong-credentials-has-been-input","title":"Reconfiguration of the GCP service account attributes if wrong credentials has been input","text":"<p>Modify the value of a specific flag if changes are needed:</p> <pre><code>rmk config init --google-application-credentials &lt;path_to_new_exported_google_service_account_file&gt;\n</code></pre>"},{"location":"configuration/configuration-management/init-k3d-provider/","title":"Initialization of K3D cluster provider","text":""},{"location":"configuration/configuration-management/init-k3d-provider/#list-of-main-attributes-of-the-rmk-configuration","title":"List of main attributes of the RMK configuration","text":"<pre><code>name: rmk-test-develop # RMK config name, a unique identifier which consists of the project (tenant) name and the abbreviated name of the Git branch.\ntenant: rmk-test # Tenant name, which is equivalent to the project name.\nenvironment: develop # Environment name.\nroot-domain: rmk-test-develop.edenlab.dev # Root domain name used across the cluster.\ncluster-provider: k3d # Selected cluster provider.\n# ...\n</code></pre>"},{"location":"configuration/configuration-management/init-k3d-provider/#prerequisites","title":"Prerequisites","text":"<ol> <li>Create a separate ephemeral branch, e.g.: <code>feature/&lt;issue_key&gt;-&lt;issue_number&gt;-&lt;issue_description&gt;</code>.</li> <li>Initialize configuration    for this branch with the <code>localhost</code> root domain name.</li> </ol>"},{"location":"configuration/configuration-management/init-k3d-provider/#configuration","title":"Configuration","text":"<p>K3D is the default cluster provider in RMK.</p> <p>To initialize RMK configuration for a K3D cluster, run:</p> <pre><code>rmk config init\n</code></pre> <p>K3D is intended for provisioning local clusters, primarily for development environments.</p>"},{"location":"configuration/configuration-management/init-onprem-provider/","title":"Initialization of On-Premise cluster provider","text":""},{"location":"configuration/configuration-management/init-onprem-provider/#list-of-main-attributes-of-the-rmk-configuration","title":"List of main attributes of the RMK configuration","text":"<pre><code>name: rmk-test-develop # RMK config name, a unique identifier which consists of the project (tenant) name and the abbreviated name of the Git branch.\ntenant: rmk-test # Tenant name, which is equivalent to the project name.\nenvironment: develop # Environment name.\nroot-domain: rmk-test-develop.edenlab.dev # Root domain name used across the cluster.\ncluster-provider: onprem # Selected cluster provider.\n# ...\nonprem:\n  ssh-init-server-host: 10.1.1.10 # K3S init server host.\n  ssh-private-key: /home/user/.ssh/id_rsa # Absolute path to SSH private key file. \n  ssh-user: user # SSH user.\n# ...\n</code></pre>"},{"location":"configuration/configuration-management/init-onprem-provider/#prerequisites","title":"Prerequisites","text":"<p> Important</p> <p>The On-Premise provider is fundamentally  different from cloud providers like AWS, Azure, GCP.</p> <p>It requires manual preparation and configuration the underlying machines (bare-metal or VMs), low-level understanding of how Kubernetes and K3S behave in a non-cloud setup (control plane bootstrap, agents joining, resource planning, scheduling, etc.).</p> <p>Additionally, you must ensure correct networking rules (firewall, routing, node-to-node connectivity).</p> <ol> <li> <p>OS:</p> <ul> <li>Only Linux machines with    systemd are supported.</li> <li>Tested distributions:    RHEL 9,    Ubuntu 22.04,    Debian 12.</li> </ul> </li> <li> <p>Disk and filesystem:</p> <ul> <li>Nodes should provide a dedicated disk or partition for container storage (optional but recommended).</li> </ul> </li> <li> <p>System user:</p> <ul> <li>Shared system user must exist on all cluster nodes.</li> <li>Requires sudo privileges without password prompt.</li> </ul> </li> <li> <p>SSH access:</p> <ul> <li>SSH connectivity from the administrator machine to all cluster nodes must be available.</li> <li>SSH authentication is done via a private key.</li> <li>An absolute path to the private key must be specified    during configuration initialization,    or RMK will attempt to use the default path (e.g., <code>~/.ssh/id_[ed25519|rsa|ecdsa|dsa]</code>).</li> </ul> </li> <li> <p>Networking:</p> <ul> <li>Firewall must allow full bidirectional connectivity between all cluster nodes.</li> <li>Required ports include (but are not limited to):<ul> <li>6443/TCP (Kubernetes API server)</li> <li>10250/TCP (Kubelet API)</li> <li>8472/UDP (Flannel VXLAN overlay network)</li> <li>51820/UDP, 51821/UDP (WireGuard, if enabled)</li> </ul> </li> </ul> </li> <li> <p>DNS resolution:</p> <ul> <li>If hostnames are used in the operator/K3S\u2019s configuration, they must resolve to correct internal IPs using    DNS.</li> <li>Alternatively, configure /etc/hosts on all cluster nodes.</li> </ul> </li> <li> <p>K3S init server host:</p> <ul> <li>An IP address must be allocated for the bootstrap control    plane node (used by other nodes when joining the cluster).</li> </ul> </li> </ol>"},{"location":"configuration/configuration-management/init-onprem-provider/#configuration","title":"Configuration","text":"<p>The 2 supported configuration scenarios are:</p> <ul> <li>via RMK flags:   <pre><code>rmk config init --cluster-provider=onprem \\\n  --onprem-ssh-init-server-host=&lt;k3s_init_server_ip&gt; \\\n  --onprem-ssh-private-key=&lt;ssh_private_key_path&gt; \\\n  --onprem-ssh-user=&lt;ssh_user&gt;\n</code></pre></li> </ul> <ul> <li>via environment variables: <code>RMK_ONPREM_SSH_INIT_SERVER_HOST</code>, <code>RMK_ONPREM_SSH_PRIVATE_KEY</code>, <code>RMK_ONPREM_SSH_USER</code>.   <pre><code>export RMK_ONPREM_SSH_INIT_SERVER_HOST=&lt;k3s_init_server_ip&gt;\nexport RMK_ONPREM_SSH_PRIVATE_KEY=&lt;ssh_private_key_path&gt;\nexport RMK_ONPREM_SSH_USER=&lt;ssh_user&gt;\nrmk config init --cluster-provider=onprem\n</code></pre></li> </ul> <p>If an environment variable or flag specifies a custom SSH private key, RMK will copy that key into the default SSH location (using the same file name), and will use it for subsequent operations.</p> <p>If CLI flags are provided, RMK will prioritize them over environment variables, as CLI flags take precedence.</p>"},{"location":"configuration/configuration-management/init-onprem-provider/#reconfiguration-of-the-on-premise-ssh-private-key","title":"Reconfiguration of the On-Premise SSH private key","text":"<p>Modify the value of a specific flag if changes are needed:</p> <pre><code>rmk config init --onprem-ssh-private-key=&lt;new_ssh_private_key_path&gt;\n</code></pre>"},{"location":"configuration/project-management/dependencies-management-and-project-inheritance/","title":"Dependencies management and project inheritance","text":""},{"location":"configuration/project-management/dependencies-management-and-project-inheritance/#overview","title":"Overview","text":"<p>To work with the RMK project\u2019s repository, RMK needs to resolve and install additional dependencies that are described in the project.yaml file. The inheritance configuration of the upstream project\u2019s repository is defined in the <code>project.dependencies</code> section of the project.yaml file. All inherited upstream project repositories will be loaded into the <code>.PROJECT</code> directory in the root directory according to the sections described in the project.yaml file.</p> <p>To override inherited versions of dependencies and add-ons described in the inventory, you need to specify the entire block with all the required fields.</p> <pre><code>inventory:\n  # ...\n  hooks:\n    helmfile.hooks.infra:\n      version: v1.18.0\n      url: git::https://github.com/&lt;owner&gt;/{{.Name}}.git?ref={{.Version}}\n  # ...\n</code></pre> <p>Dependency resolution occurs when executing almost any RMK command, except for those in the <code>rmk project</code> command category.</p>"},{"location":"configuration/project-management/dependencies-management-and-project-inheritance/#change-dependency-versions-of-the-inherited-projects-repository","title":"Change dependency versions of the inherited project\u2019s repository","text":"<p>Find the <code>project</code> section in the project.yaml file and change the <code>version</code> value to the needed stable tag. For example:</p> <pre><code>project:\n  dependencies:\n    # ...\n    - name: &lt;upstream_repository_prefix&gt;.bootstrap.infra\n      version: v2.17.0 # e.g., a different version of the dependency is required by this project\n      url: git::https://github.com/&lt;owner&gt;/{{.Name}}.git?ref={{.Version}}\n    # ...\n</code></pre> <p>Then, in the <code>helmfiles</code> section of the <code>helmfile.yaml.gotmpl</code> file the {{ env \u201cHELMFILE_&lt;project_name&gt;_PATHS\u201d }} environment variable will be used, this way RMK will manage the dependencies of the nested <code>Helmfile</code>s.</p> <p>The variable name is formed according to the following template: <code>HELMFILE_&lt;project_name&gt;_PATHS</code>. This mechanism is necessary for resolving circular dependencies correctly.</p>"},{"location":"configuration/project-management/dependencies-management-and-project-inheritance/#change-inherited-versions-of-helmfile-hooks","title":"Change inherited versions of Helmfile hooks","text":"<p>RMK allows to avoid controlling the versioning of the <code>Helmfile</code> hooks through the project.yaml file of the downstream project\u2019s repository, instead of it, RMK allows inheriting these version hooks from the upstream project\u2019s repository. It also supports multi-versioning of the <code>Helmfile</code> hooks as part of the inheritance from several upstream projects by a downstream project.</p> <p>In order for these features to work, you need to use the {{ env \u201cHELMFILE_&lt;current_project_name&gt;_HOOKS_DIR\u201d }} variable in <code>helmfile.yaml.gotmpl</code>. For example:</p> <pre><code>commonLabels:\n  # ...\n  bin: {{ env \"HELMFILE_RMK_TEST_HOOKS_DIR\" }}/bin\n# ...\n</code></pre> <p>Let\u2019s look at the following examples of the inheritance:</p> <ol> <li> <p>Hook version inheritance from the upstream project\u2019s repository:</p> <p>The project.yaml file of the downstream project is the following:</p> <pre><code>project:\n  dependencies:\n    - name: rmk-test.bootstrap.infra\n      version: v0.1.0\n      url: git::https://github.com/edenlabllc/{{.Name}}.git?ref={{.Version}}\n    # ...\n</code></pre> <p>In this case, a version of the Helmfile hooks in the <code>inventory.hooks</code> section is not specified, however,  it is indicated that the current project of the repository inherits <code>rmk-test.bootstrap.infra</code> with the <code>v0.1.0</code>  version.  In turn, <code>rmk-test.bootstrap.infra</code> inherits  the cluster-deps.bootstrap.infra repository.  The project.yaml file for the <code>rmk-test.bootstrap.infra</code>  repository is also missing the version of the hooks:</p> <pre><code>project:\n  dependencies:\n    - name: cluster-deps.bootstrap.infra\n      version: v0.1.0\n      url: git::https://github.com/edenlabllc/{{.Name}}.git?ref={{.Version}}\n    # ...\n</code></pre> <p>Also, the project.yaml file of the <code>cluster-deps.bootstrap.infra</code>  repository will contain the version of the <code>Helmfile</code> hooks,  which will finally be inherited by the downstream project\u2019s repository.</p> <pre><code>inventory:\n  # ...\n  hooks:\n    helmfile.hooks.infra:\n      version: v1.29.1\n      url: git::https://github.com/edenlabllc/{{.Name}}.git?ref={{.Version}}\n  # ...\n</code></pre> <p>There is no <code>project.dependencies</code> section in the project.yaml  file of the <code>cluster-deps.bootstrap.infra</code> repository, since there is no inheritance.</p> <p>This configuration scheme is the most common and has the following inheritance scheme for the <code>Helmfile</code> hooks:</p> <pre><code>Project repo name:            cluster-deps.bootstrap.infra -&gt; rmk-test.bootstrap.infra -----&gt; &lt;downstream_project&gt;.bootstrap.infra\nProject repo version:         v0.1.0                          v0.1.0                          &lt;downstream_project_version&gt;\nHooks repo name with version: helmfile.hooks.infra-v1.29.1 -&gt; helmfile.hooks.infra-v1.29.1 -&gt; helmfile.hooks.infra-v1.29.1\n</code></pre> </li> <li> <p>Hook version inheritance from the upstream project\u2019s repository in case the <code>rmk-test</code> project has a fixed version    of the <code>Helmfile</code> hooks specified in its project.yaml file:</p> <p>The project.yaml file of the downstream project is the following:</p> <pre><code>project:\n  dependencies:\n    - name: rmk-test.bootstrap.infra\n      version: v0.1.0\n      url: git::https://github.com/edenlabllc/{{.Name}}.git?ref={{.Version}}\n    # ...\n</code></pre> <p>In this case, the version of the Helmfile hooks in the <code>inventory.hooks</code> section is not specified,  however, it is indicated that the current project of the repository inherits <code>rmk-test.bootstrap.infra</code> with  the <code>v0.1.0</code> version.  In turn, <code>rmk-test.bootstrap.infra</code> inherits  the cluster-deps.bootstrap.infra  repository which already has its own fixed version of <code>v1.29.0</code> of the <code>Helmfile</code> hooks in the <code>inventory.hooks</code>  section:</p> <pre><code>project:\n  dependencies:\n    - name: cluster-deps.bootstrap.infra\n      version: v0.1.0\n      url: git::https://github.com/edenlabllc/{{.Name}}.git?ref={{.Version}}\n# ...\ninventory:\n  # ...\n  hooks:\n    helmfile.hooks.infra:\n      version: v1.29.0\n      url: git::https://github.com/edenlabllc/{{.Name}}.git?ref={{.Version}}\n  # ...\n</code></pre> <p>The project.yaml file of the <code>cluster-deps.bootstrap.infra</code>  repository will contain the version of the <code>Helmfile</code> hooks,  which will be inherited by the downstream project\u2019s repository:</p> <pre><code>inventory:\n  # ...\n  hooks:\n    helmfile.hooks.infra:\n      version: v1.29.1\n      url: git::https://github.com/edenlabllc/{{.Name}}.git?ref={{.Version}}\n  # ...\n</code></pre> <p>This configuration scheme will look like this:</p> <pre><code>Project repo name:            cluster-deps.bootstrap.infra -&gt; rmk-test.bootstrap.infra -----&gt; &lt;downstream_project&gt;.bootstrap.infra\nProject repo version:         v0.1.0                          v0.1.0                          &lt;downstream_project_version&gt;\nHooks repo name with version: helmfile.hooks.infra-v1.29.1 -&gt; helmfile.hooks.infra-v1.29.0 -&gt; helmfile.hooks.infra-v1.29.1\n</code></pre> <p>The downstream project\u2019s repository will inherit the latest version of <code>Helmfile</code> hooks, specifically from  the <code>cluster-deps.bootstrap.infra</code> repository. As a result, in the downstream project\u2019s repository, we will have the two loaded versions of <code>Helmfile</code> hooks:</p> <ul> <li>One will be relevant for the <code>cluster-deps.bootstrap.infra</code> repository and the downstream project\u2019s repository.</li> <li>Another will be relevant for the <code>rmk-test.bootstrap.infra</code> repository.</li> </ul> <p>This mechanism allows for multi-versioning support of the <code>Helmfile</code> hooks at different levels of the inheritance.</p> </li> <li> <p>Hook version inheritance from the upstream project\u2019s repository in case the downstream project    has a fixed version of cluster-deps.bootstrap.infra    specified in its project.yaml file:</p> <p>The project.yaml file of the downstream project is the following:</p> <pre><code>project:\n  dependencies:\n    - name: cluster-deps.bootstrap.infra\n      version: v0.2.0\n      url: git::https://github.com/edenlabllc/{{.Name}}.git?ref={{.Version}}       \n    - name: rmk-test.bootstrap.infra\n      version: v0.1.0\n      url: git::https://github.com/edenlabllc/{{.Name}}.git?ref={{.Version}}\n    # ...\n</code></pre> <p>The dependencies should be declared in the correct order of inheritance: the first one  is <code>cluster-deps.bootstrap.infra</code>, then <code>rmk-test.bootstrap.infra</code>, then other repositories (if needed).</p> <p>In this case, a version of the <code>Helmfile</code> hooks in the <code>inventory.hooks</code> section is not specified,  however, it is indicated that the current project of the repository inherits <code>rmk-test.bootstrap.infra</code> with  the <code>v0.1.0</code> version.  In turn, <code>rmk-test.bootstrap.infra</code> inherits the <code>cluster-deps.bootstrap.infra</code> repository.  The project.yaml file for the <code>rmk-test.bootstrap.infra</code>  repository is also missing the version of the hooks:</p> <pre><code>project:\n  dependencies:\n    - name: cluster-deps.bootstrap.infra\n      version: v0.1.0\n      url: git::https://github.com/edenlabllc/{{.Name}}.git?ref={{.Version}}\n    # ...\n</code></pre> <p>The project.yaml file of the <code>cluster-deps.bootstrap.infra</code>  repository  of the <code>v0.2.0</code> version will contain the version of the <code>Helmfile</code> hooks, which will be inherited by the downstream  projects:</p> <pre><code>inventory:\n  # ...\n  hooks:\n    helmfile.hooks.infra:\n      version: v1.30.0\n      url: git::https://github.com/edenlabllc/{{.Name}}.git?ref={{.Version}}\n  # ...\n</code></pre> <p>This configuration scheme will look like this:</p> <pre><code>Project repo name:            cluster-deps.bootstrap.infra -&gt; rmk-test.bootstrap.infra -----&gt; &lt;downstream_project&gt;.bootstrap.infra\nProject repo version:         v0.2.0                          v0.1.0                          &lt;downstream_project_version&gt;\nHooks repo name with version: helmfile.hooks.infra-v1.30.0 -&gt; helmfile.hooks.infra-v1.30.0 -&gt; helmfile.hooks.infra-v1.30.0\n</code></pre> <p>Since the downstream project\u2019s repositories inherit the <code>Helmfile</code> hooks from the <code>cluster-deps.bootstrap.infra</code> repository, and we redefined the <code>cluster-deps.bootstrap.infra</code> dependency in the downstream project\u2019s, all repositories will inherit this concrete version, and only it will be downloaded.</p> </li> </ol>"},{"location":"configuration/project-management/dependencies-management-and-project-inheritance/#change-inherited-versions-of-helm-plugins-tools","title":"Change inherited versions of Helm plugins, tools","text":"<p>The same inheritance method as for the <code>Helmfile</code> hooks is supported for the <code>inventory</code>\u2019s <code>helm-plugins</code> and <code>tools</code> sections.</p> <p>If a specific version is not specified, the latest version from the upstream project\u2019s repository  will be used. In this case, multi-versioning is not supported, and only one version  will be downloaded.</p> <p>All add-ons versions in the <code>inventory</code> section must be specified in the SemVer2 format, as the inheritance mechanism relies on this format to distinguish the version order.</p>"},{"location":"configuration/project-management/preparation-of-project-repository/","title":"Preparation of the project repository","text":""},{"location":"configuration/project-management/preparation-of-project-repository/#prerequisites","title":"Prerequisites","text":"<ul> <li>Create a remote repository (Git) for a project in your Version Control System (e.g., GitHub)   according to the requirements. For   example: <code>rmk-test.bootstrap.infra</code></li> <li> <p>Clone the existing project repository:</p> <pre><code>git clone &lt;repo_url&gt;\n</code></pre> <p>Alternatively, initialize a new repository manually:</p> <pre><code>git init\ngit remote add &lt;repo_name&gt; &lt;repo_url&gt;\ngit commit --allow-empty --message \"Initial commit\"\n</code></pre> <p>RMK requires a Git branch with at least one commit and a configured <code>origin</code> remote to correctly resolve the project name and environment.</p> </li> </ul> <ul> <li>Checkout the required branch. For example: <code>develop</code>.</li> </ul>"},{"location":"configuration/project-management/preparation-of-project-repository/#automatic-generation-of-the-project-structure-from-scratch","title":"Automatic generation of the project structure from scratch","text":"<p>RMK supports automatic generation of the project structure from scratch, according to the presented project specification described in project.yaml file.</p> <p>Use the following command:</p> <pre><code>rmk project generate --environment=\"develop.root-domain=&lt;custom_root_domain_name&gt;\" \\\n  --owner=gh_user --scope=&lt;upstream_project_name&gt; \\\n  --scope=&lt;downstream_project_name&gt; \n</code></pre> <p>Add the <code>--create-sops-age-keys</code> flag if you want to create the project structure along with SOPS age private keys.</p> <p>This will create a default project structure and set up an example release based on Nginx.</p> <p>If the <code>project.yaml</code> file is missing, it will be automatically created by the command.</p>"},{"location":"configuration/project-management/preparation-of-project-repository/#projectyaml","title":"project.yaml","text":"<p>The <code>project.yaml</code> file is the main configuration file of the repository, the file is used by RMK and contains the following main sections:</p> <ul> <li> <p><code>project</code>: Optional, contains a list of dependencies of the upstream project\u2019s repositories and the project   specification.</p> <pre><code>project:\n  # Optional, needed if you want to add the dependencies with upstream projects to the downstream project.\n  dependencies:\n      # Required, dependencies upstream project's repository name.\n    - name: &lt;upstream_repository_prefix&gt;.bootstrap.infra\n      # Required, dependencies upstream project's repository version in `SemVer2` format, also can be a branch name or a commit hash.\n      version: &lt;SemVer2&gt;\n      # Required, dependencies upstream project's repository URL.\n      url: git::https://github.com/&lt;owner&gt;/{{.Name}}.git?ref={{.Version}}    \n  # Optional, needed if you want automatic generation of the project structure from scratch.\n  spec:\n    # Required, list of available environments with specific root domain name (Git branches). \n    environments:\n      - develop:\n          root-domain: &lt;custom_name&gt;.example.com\n      - production:\n          root-domain: &lt;custom_name&gt;.example.com\n      - staging:\n          root-domain: &lt;custom_name&gt;.example.com\n    # Optional, list of owners of the project.\n    owners:\n      - &lt;owner_1&gt;\n      - &lt;owner_2&gt;\n    # Required, list of available scope of the project.\n    scopes:\n      - &lt;upstream_project_name&gt;\n      - &lt;downstream_project_name&gt;\n# ... \n</code></pre> </li> </ul> <ul> <li> <p><code>inventory</code>: Optional, contains a map of the extra configurations required to launch the project.</p> <pre><code>inventory:\n  # Optional, contains a map of the Helm plugins repositories.\n  helm-plugins:\n    # Optional, Helm plugin name.\n    diff:\n      # Required, Helm plugin version in the `SemVer2` format.\n      version: &lt;SemVer2&gt;\n      # Required, Helm plugin repository URL.\n      url: https://github.com/&lt;owner&gt;/helm-diff\n    # ...\n  # Optional, contains a map of the Helmfile hooks repositories with shell scripts.\n  hooks:\n    # Optional, Helmfile hooks repository name.\n    helmfile.hooks.infra:\n      # Required, Helmfile hooks repository version in the `SemVer2` format.\n      version: &lt;SemVer2&gt;\n      # Required, Helmfile hooks repository URL.\n      url: git::https://github.com/&lt;owner&gt;/{{.Name}}.git?ref={{.Version}}\n  # Optional, contains a map of the sources of binary file tools.\n  tools:\n    # Optional, tool name.\n    clusterctl:\n      # Required, tool version in `SemVer2` format.\n      version: &lt;SemVer2&gt;\n      # Required, tool source URL.\n      url: https://github.com/kubernetes-sigs/cluster-api/releases/download/v{{.Version}}/{{.Name}}-{{.Os}}-amd64\n      # Optional, specific key overrides for the described OS name.\n      os-linux: linux\n      os-mac: darwin\n      # Optional, an option that allows to rename the downloaded binary file by the tool name.\n      rename: true\n    # ...\n</code></pre> </li> </ul> Example of the full <code>project.yaml</code> file <pre><code>project:\n  dependencies:\n    - name: cluster-deps.bootstrap.infra\n      version: v0.1.0\n      url: git::https://github.com/edenlabllc/{{.Name}}.git?ref={{.Version}}\n  spec:\n    environments:\n      - develop:\n          root-domain: localhost\n      - production:\n          root-domain: localhost\n      - staging:\n          root-domain: localhost\n    owners:\n      - owner1\n      - owner2\n    scopes:\n      - deps\n      - project1\ninventory:\n  helm-plugins:\n    diff:\n      version: v3.8.1\n      url: https://github.com/databus23/helm-diff\n    helm-git:\n      version: v0.15.1\n      url: https://github.com/aslafy-z/helm-git\n    secrets:\n      version: v4.5.0\n      url: https://github.com/jkroepke/helm-secrets\n  hooks:\n    helmfile.hooks.infra:\n      version: v1.29.1\n      url: git::https://github.com/edenlabllc/{{.Name}}.git?ref={{.Version}}\n  tools:\n    clusterctl:\n      version: 1.7.4\n      url: https://github.com/kubernetes-sigs/cluster-api/releases/download/v{{.Version}}/{{.Name}}-{{.Os}}-amd64\n      os-linux: linux\n      os-mac: darwin\n      rename: true\n    kubectl:\n      version: 1.28.13\n      url: https://dl.k8s.io/release/v{{.Version}}/bin/{{.Os}}/amd64/{{.Name}}\n      checksum: https://dl.k8s.io/release/v{{.Version}}/bin/{{.Os}}/amd64/{{.Name}}.sha256\n      os-linux: linux\n      os-mac: darwin\n    helm:\n      version: 3.10.3\n      url: https://get.helm.sh/{{.Name}}-v{{.Version}}-{{.Os}}-amd64.tar.gz\n      checksum: https://get.helm.sh/{{.Name}}-v{{.Version}}-{{.Os}}-amd64.tar.gz.sha256sum\n      os-linux: linux\n      os-mac: darwin\n    helmfile:\n      version: 0.157.0\n      url: https://github.com/{{.Name}}/{{.Name}}/releases/download/v{{.Version}}/{{.Name}}_{{.Version}}_{{.Os}}_amd64.tar.gz\n      checksum: https://github.com/{{.Name}}/{{.Name}}/releases/download/v{{.Version}}/{{.Name}}_{{.Version}}_checksums.txt\n      os-linux: linux\n      os-mac: darwin\n    sops:\n      version: 3.8.1\n      url: https://github.com/getsops/{{.Name}}/releases/download/v{{.Version}}/{{.Name}}-v{{.Version}}.{{.Os}}\n      os-linux: linux.amd64\n      os-mac: darwin\n      rename: true\n    age:\n      version: 1.1.1\n      url: https://github.com/FiloSottile/{{.Name}}/releases/download/v{{.Version}}/{{.Name}}-v{{.Version}}-{{.Os}}-amd64.tar.gz\n      os-linux: linux\n      os-mac: darwin\n    k3d:\n      version: 5.7.3\n      url: https://github.com/k3d-io/{{.Name}}/releases/download/v{{.Version}}/{{.Name}}-{{.Os}}-amd64\n      os-linux: linux\n      os-mac: darwin\n      rename: true\n    yq:\n      version: 4.35.2\n      url: https://github.com/mikefarah/{{.Name}}/releases/download/v{{.Version}}/{{.Name}}_{{.Os}}_amd64\n      os-linux: linux\n      os-mac: darwin\n      rename: true\n    aws-iam-authenticator:\n      version: 0.6.27\n      url: https://github.com/kubernetes-sigs/{{.Name}}/releases/download/v{{.Version}}/{{.Name}}_{{.Version}}_{{.Os}}_amd64\n      os-linux: linux\n      os-mac: darwin\n      rename: true\n    gke-auth-plugin:\n      version: 0.1.1\n      url: https://github.com/traviswt/{{.Name}}/releases/download/{{.Version}}/{{.Name}}_{{.Os}}_x86_64.tar.gz\n      os-linux: Linux\n      os-mac: Darwin\n</code></pre> <p>The project file\u2019s <code>inventory</code> section supports placeholders, they are required for correct URL formation.</p> <ul> <li>{{.Name}}: Replaced with the key\u2019s value.</li> <li>{{.Version}}: Replaced with the <code>version</code> field.</li> <li>{{.HelmfileTenant}}: Replaced with the tenant (project) name for the Helmfile selected from the list.</li> <li>{{.Os}}: Replaced with the values from the <code>os-linux</code>, <code>os-mac</code> fields according to the specific operating system,   where RMK is run.</li> </ul> <p>The field <code>rename</code> of the boolean type is required to correct the name of the binary file of the downloaded tool according to the value of the <code>name</code> field. This is mainly required for the cases, when the artifact is not the archive. For example:</p> <ul> <li>The initial file name after the download: <code>helmfile_darwin_amd64</code>.</li> <li>After applying the <code>rename</code> instruction it gets a value of the <code>name</code> field: <code>helmfile</code>.</li> </ul>"},{"location":"configuration/project-management/requirement-for-project-repository/","title":"Requirement for project repository","text":""},{"location":"configuration/project-management/requirement-for-project-repository/#overview","title":"Overview","text":"<ol> <li>The name of the project repository should consist of the following parts: <code>&lt;project_name&gt;</code>.<code>&lt;custom_suffix&gt;</code>.    For example: <code>rmk-test.bootstrap.infra</code> or <code>rmk-test.infra</code>.</li> <li> <p>The project\u2019s repository exists within the GitLab Flow only    and therefore supports the following set of static branches:</p> <ul> <li><code>develop</code></li> <li><code>staging</code></li> <li><code>production</code></li> </ul> <p>Each branch corresponds to its own environment with a separately deployed Kubernetes cluster. </p> <p>RMK supports these branches as well as the ephemeral branches:</p> <ul> <li> <p>A <code>feature</code> branch should have the following naming: <code>feature/&lt;issue_key&gt;-&lt;issue_number&gt;-&lt;issue_description&gt;</code>    (<code>develop</code> configuration will be used).</p> <p>For example: <code>feature/RMK-123-new-feature</code>.</p> <p>RMK will use <code>&lt;issue_key&gt;</code> and <code>&lt;issue_number&gt;</code> as the cluster name.    - A <code>release</code> branch should have the following naming: <code>release/&lt;SemVer2&gt;-rc</code> (<code>staging</code> configuration will be used)  or <code>release/&lt;SemVer2&gt;</code> (<code>production</code> configuration will be used).</p> <p>For example: <code>release/v1.0.0</code>. </p> <p>RMK will use the project name and the <code>&lt;SemVer2&gt;</code> tag as the cluster name.    - A <code>hotfix</code> branch should have the following naming: <code>hotfix/&lt;SemVer2&gt;</code> (<code>production</code> configuration will be used).</p> <p>For example: <code>hotfix/v1.0.1</code>. </p> <p>RMK will use the project name and the <code>&lt;SemVer2&gt;</code> tag as the cluster name.</p> </li> </ul> </li> </ol>"},{"location":"configuration/project-management/requirement-for-project-repository/#expected-repository-structure","title":"Expected repository structure:","text":"<pre><code>etc/&lt;upstream_project_name&gt;/&lt;environment&gt;/secrets/\n  .sops.yaml # The public key for the current set of secrets.\n  .spec.yaml.gotmpl # The secrets template for generating new or rotating current secrets.\n  &lt;release name&gt;.yaml # Values containing release secrets for a specific environment.\netc/&lt;upstream_project_name&gt;/&lt;environment&gt;/values/\n  &lt;release name&gt;.yaml # Values containing release configuration for a specific environment.\n  &lt;release name&gt;.yaml.gotmpl # Values containing the release configuration for a specific environment using the Golang templates.\netc/&lt;upstream_project_name&gt;/&lt;environment&gt;/\n  releases.yaml # Release specification for installation of the charts.\n  globals.yaml # Set of global values within a specific scope.\n  globals.yaml.gotmpl # Set of global values within a specific scope using the Golang templates.\netc/&lt;downstream_project_name&gt;/&lt;environment&gt;/secrets/\n  .sops.yaml  # -//-\n  .spec.yaml.gotmpl # -//-\n  &lt;release name&gt;.yaml # -//-\netc/&lt;downstream_project_name&gt;/&lt;environment&gt;/secrets/\n  &lt;release name&gt;.yaml # -//-\n  &lt;release name&gt;.yaml.gotmpl # -//-\netc/&lt;downstream_project_name&gt;/&lt;environment&gt;/\n  releases.yaml # -//-\n  globals.yaml # -//-\n  globals.yaml.gotmpl # -//-\nhelmfile.yaml.gotmpl # Helmfile describing the release process for specific project releases using the Golang templates.\nproject.yaml # Project specification for the dependencies and inventory installed via RMK.\n</code></pre>"},{"location":"configuration/project-management/requirement-for-project-repository/#files-for-managing-releases-and-their-values-at-the-scope-level","title":"Files for managing releases and their values at the scope level","text":""},{"location":"configuration/project-management/requirement-for-project-repository/#requirement-for-releasesyaml","title":"Requirement for <code>releases.yaml</code>","text":"<pre><code>&lt;release_name_foo&gt;: # Required, release name from helmfile.yaml.gotmpl.\n  enabled: true # Required, enable|disable release from helmfile.yaml.gotmpl.\n  image: # Optional, needed when using a private container image with the automatic release update feature of RMK.\n    repository: &lt;full_container_images_repository_url&gt;\n    tag: &lt;container_images_tag&gt;\n&lt;release_name_bar&gt;: # -//-\n  enabled: false # -//-\n# ...\n</code></pre> <p><code>releases.yaml</code> cannot be used as a template, all the values must be defined explicitly.</p>"},{"location":"configuration/project-management/requirement-for-project-repository/#requirement-for-globalsyamlgotmpl","title":"Requirement for <code>globals.yaml.gotmpl</code>","text":"<pre><code># configs - enumeration of configurations divided into sets related to the Kubernetes ConfigMaps.\nconfigs:\n  auditLog: |\n    {{- readFile (printf \"%s/audit-log.json\" \"values/configs\") | nindent 4 }}\n  # ...\n\n# envs - enumeration of environment variables divided into sets related to the Kubernetes environment variables for the containers.\nenvs:\n  # The global environment variable used by multiple releases\n  FOO: false\n  # ...\n\n# hooks - enumeration of environment variables divided into sets related to the Helmfile hooks arguments.\nhooks:\n  &lt;release_name&gt;:\n    common-postuninstall-hook:\n      events:\n        - postuninstall\n      showlogs: true\n      command: \"{{`{{ .Release.Labels.bin }}`}}/common-postuninstall-hook.sh\"\n      args:\n        - \"{{`{{ .Release.Namespace }}`}}\"\n  # ...\n</code></pre> <p><code>globals.yaml.gotmpl</code> is used in two cases:</p> <ol> <li>When values, configurations or environment variables need to be declared globally for multiple releases.</li> <li>When the current project is planned to be inherited by a downstream project and the overrides should be      supported.</li> </ol>"},{"location":"configuration/project-management/requirement-for-project-repository/#requirement-for-helmfileyamlgotmpl","title":"Requirement for <code>helmfile.yaml.gotmpl</code>","text":"<p>All sections in <code>helmfile.yaml.gotmpl</code> must be properly defined for RMK to function correctly.</p> Example of the <code>helmfile.yaml.gotmpl</code> file <pre><code>environments:\n  local:\n  develop:\n    missingFileHandler: Warn\n    values:\n      - etc/&lt;project_name&gt;/{{ .Environment.Name }}/globals.yaml\n      - etc/&lt;project_name&gt;/{{ .Environment.Name }}/globals.yaml.gotmpl\n      - etc/&lt;project_name&gt;/{{ .Environment.Name }}/releases.yaml\n      {{- if eq (env \"K3D_CLUSTER\") \"true\" }}\n      - etc/&lt;project_name&gt;/{{ .Environment.Name }}/values/k3d/releases.yaml\n      {{- end }}\n      - {{ requiredEnv \"PWD\" }}/etc/&lt;project_name&gt;/{{ .Environment.Name }}/globals.yaml\n      - {{ requiredEnv \"PWD\" }}/etc/&lt;project_name&gt;/{{ .Environment.Name }}/globals.yaml.gotmpl\n      - {{ requiredEnv \"PWD\" }}/etc/&lt;project_name&gt;/{{ .Environment.Name }}/releases.yaml\n      {{- if eq (env \"K3D_CLUSTER\") \"true\" }}\n      - {{ requiredEnv \"PWD\" }}/etc/&lt;project_name&gt;/{{ .Environment.Name }}/values/k3d/releases.yaml\n      {{- end }}\n  production: \n    missingFileHandler: Warn\n    values:\n      - etc/&lt;project_name&gt;/{{ .Environment.Name }}/globals.yaml\n      - etc/&lt;project_name&gt;/{{ .Environment.Name }}/globals.yaml.gotmpl \n      - etc/&lt;project_name&gt;/{{ .Environment.Name }}/releases.yaml\n      {{- if eq (env \"K3D_CLUSTER\") \"true\" }}\n      - etc/&lt;project_name&gt;/{{ .Environment.Name }}/values/k3d/releases.yaml\n      {{- end }}\n      - {{ requiredEnv \"PWD\" }}/etc/&lt;project_name&gt;/{{ .Environment.Name }}/globals.yaml\n      - {{ requiredEnv \"PWD\" }}/etc/&lt;project_name&gt;/{{ .Environment.Name }}/globals.yaml.gotmpl\n      - {{ requiredEnv \"PWD\" }}/etc/&lt;project_name&gt;/{{ .Environment.Name }}/releases.yaml\n      {{- if eq (env \"K3D_CLUSTER\") \"true\" }}\n      - {{ requiredEnv \"PWD\" }}/etc/&lt;project_name&gt;/{{ .Environment.Name }}/values/k3d/releases.yaml\n      {{- end }}                        \n  staging:\n    missingFileHandler: Warn\n    values:\n      - etc/&lt;project_name&gt;/{{ .Environment.Name }}/globals.yaml\n      - etc/&lt;project_name&gt;/{{ .Environment.Name }}/globals.yaml.gotmpl\n      - etc/&lt;project_name&gt;/{{ .Environment.Name }}/releases.yaml\n      {{- if eq (env \"K3D_CLUSTER\") \"true\" }}\n      - etc/&lt;project_name&gt;/{{ .Environment.Name }}/values/k3d/releases.yaml\n      {{- end }}                     \n      - {{ requiredEnv \"PWD\" }}/etc/&lt;project_name&gt;/{{ .Environment.Name }}/globals.yaml\n      - {{ requiredEnv \"PWD\" }}/etc/&lt;project_name&gt;/{{ .Environment.Name }}/globals.yaml.gotmpl\n      - {{ requiredEnv \"PWD\" }}/etc/&lt;project_name&gt;/{{ .Environment.Name }}/releases.yaml\n      {{- if eq (env \"K3D_CLUSTER\") \"true\" }}\n      - {{ requiredEnv \"PWD\" }}/etc/&lt;project_name&gt;/{{ .Environment.Name }}/values/k3d/releases.yaml\n      {{- end }}\n---\nhelmDefaults:\nwait: true\nwaitForJobs: true\ntimeout: 3600\n\n# The set of paths for the inherited Helmfiles is controlled through the project.yaml file using RMK.\n# DO NOT EDIT the \"helmfiles\" field's values.\nhelmfiles: {{ env \"HELMFILE_&lt;project_name&gt;_PATHS\" }}\n\nmissingFileHandler: Warn\n\ncommonLabels:\n  scope: &lt;project_name&gt;\n  bin: {{ env \"HELMFILE_&lt;project_name&gt;_HOOKS_DIR\" }}/bin\n\ntemplates:\n  release:\n    createNamespace: true\n    labels:\n      app: \"{{`{{ .Release.Name }}`}}\"\n    missingFileHandler: Warn\n    values:\n      - etc/{{`{{ .Release.Labels.scope }}`}}/{{`{{ .Environment.Name }}`}}/values/{{`{{ .Release.Name }}`}}.yaml.gotmpl\n      - etc/{{`{{ .Release.Labels.scope }}`}}/{{`{{ .Environment.Name }}`}}/values/{{`{{ .Release.Name }}`}}.yaml\n      {{- if eq (env \"K3D_CLUSTER\") \"true\" }}\n      - etc/{{`{{ .Release.Labels.scope }}`}}/{{`{{ .Environment.Name }}`}}/values/k3d/values/{{`{{ .Release.Name }}`}}.yaml.gotmpl\n      - etc/{{`{{ .Release.Labels.scope }}`}}/{{`{{ .Environment.Name }}`}}/values/k3d/values/{{`{{ .Release.Name }}`}}.yaml\n      {{- end }}\n      - {{ requiredEnv \"PWD\" }}/etc/{{`{{ .Release.Labels.scope }}`}}/{{`{{ .Environment.Name }}`}}/values/{{`{{ .Release.Name }}`}}.yaml.gotmpl\n      - {{ requiredEnv \"PWD\" }}/etc/{{`{{ .Release.Labels.scope }}`}}/{{`{{ .Environment.Name }}`}}/values/{{`{{ .Release.Name }}`}}.yaml\n      {{- if eq (env \"K3D_CLUSTER\") \"true\" }}\n      - {{ requiredEnv \"PWD\" }}/etc/{{`{{ .Release.Labels.scope }}`}}/{{`{{ .Environment.Name }}`}}/values/k3d/values/{{`{{ .Release.Name }}`}}.yaml.gotmpl\n      - {{ requiredEnv \"PWD\" }}/etc/{{`{{ .Release.Labels.scope }}`}}/{{`{{ .Environment.Name }}`}}/values/k3d/values/{{`{{ .Release.Name }}`}}.yaml\n      {{- end }}\n    secrets:\n      - {{ requiredEnv \"PWD\" }}/etc/{{`{{ .Release.Labels.scope }}`}}/{{`{{ .Environment.Name }}`}}/secrets/{{`{{ .Release.Name }}`}}.yaml\n\nreleases:\n  - name: &lt;release_name_foo&gt;\n    installed: {{ .Values | get (print \" &lt;release_name_foo&gt;\" \".enabled\") false }}\n</code></pre> <p>You can use the rmk project generate command to view the full example of the contents of all the project files.</p>"},{"location":"configuration/release-management/release-management/","title":"Release management","text":""},{"location":"configuration/release-management/release-management/#overview","title":"Overview","text":"<p>RMK uses Helmfile for the release management.</p> <p>RMK uses a reduced set of the <code>Helmfile</code> commands without changing their behavior. The full list of commands can be found in the release category. Additionally, flags are provided for the commands, which allow extending capabilities and help during the command execution debug.</p> <p>For example:</p> <pre><code>rmk release build\nrmk release list\nrmk release template --skip-context-switch --selector app=myapp1\nrmk release sync --helmfile-log-level=debug --selector app=myapp1 \nrmk release destroy \n</code></pre> <p>The <code>--skip-context-switch</code> (<code>-s</code>) flag can be used with commands like rmk release template to skip switching to a Kubernetes cluster.</p> <p>This is useful in situations where a cluster has not been provisioned yet, releases are being developed, but the user still wants to preview intermediate results, such as checking the templating of a release.</p> <p>In a project repository, all the release values files are stored in the <code>etc/&lt;scope&gt;/&lt;env&gt;/values/</code> directories. For example:</p> <pre><code>etc/deps/develop/values/\netc/rmk-test/staging/values/\n</code></pre> <p>The release values are inherited by the projects, e.g., the upstream project\u2019s values (deps) are included into the downstream project\u2019s values (<code>rmk-test</code>).</p> <p>All <code>releases.yaml</code> files controlling which releases are enabled/disabled are stored in the <code>etc/&lt;scope&gt;/&lt;env&gt;/</code> directories. For example:</p> <pre><code>etc/deps/develop/releases.yaml\n</code></pre> <p>Similar to the SOPS secrets files, the <code>releases.yaml</code> files are never inherited by the projects in contrast to the release values. Each project should have its own <code>releases.yaml</code> files for all deployed scopes and environments.</p> <p>The release installation order is declared in <code>helmfile.yaml.gotmpl</code> file. For an example, see the <code>cluster-deps</code> Helmfile.</p> <p>Running any of the commands in the <code>release</code> category will trigger the Helmfile\u2019s dependency resolution mechanism (DAG). Additionally, RMK verifies that the current Kubernetes context matches the Git branch and environment, preventing releases from being synchronized to an unintended Kubernetes cluster (RMK will select a correct automatically).</p> <p>Among the user-defined <code>Helmfile</code> selectors, the following labels are available by default:</p> <ul> <li><code>name</code>: Release name.</li> <li><code>namespace</code>: Release namespace.</li> <li><code>chart</code>: Chart name.</li> </ul> <p>For example:</p> <pre><code>rmk release list --selector name=myapp1\nrmk release sync --selector namespace=kube-system\nrmk release destroy --selector chart=mychart1\n</code></pre>"},{"location":"configuration/release-management/release-management/#examples-of-usage","title":"Examples of usage","text":""},{"location":"configuration/release-management/release-management/#list-of-all-available-releases","title":"List of all available releases","text":"<pre><code>rmk release list\n</code></pre>"},{"location":"configuration/release-management/release-management/#viewing-a-specific-release-yaml-after-the-helm-values-template-rendering","title":"Viewing a specific release YAML after the Helm values template rendering","text":"<pre><code>rmk release template --selector app=myapp1\n</code></pre>"},{"location":"configuration/release-management/release-management/#synchronization-of-all-releases","title":"Synchronization of all releases","text":"<pre><code>rmk release sync\n</code></pre>"},{"location":"configuration/release-management/release-management/#synchronization-of-a-specific-scope-of-the-releases","title":"Synchronization of a specific scope of the releases","text":"<pre><code>rmk release sync --selector scope=deps\n</code></pre>"},{"location":"configuration/release-management/release-management/#synchronization-of-a-specific-release-with-passing-the-set-helmfile-argument","title":"Synchronization of a specific release with passing the \u201c\u2013set\u201d Helmfile argument","text":"<pre><code>rmk release sync --selector app=myapp1 --helmfile-args=\"--set='values.key1=value1'\"\n</code></pre>"},{"location":"configuration/release-management/release-management/#destroy-all-releases","title":"Destroy all releases","text":"<pre><code>rmk release destroy\n</code></pre>"},{"location":"configuration/release-management/release-management/#overriding-release-values-for-inherited-upstream-projects","title":"Overriding release values for inherited upstream projects","text":"<p>It is possible to override any release value for the inherited upstream project repository. You can override any element separately in its YAML file.</p> <p>For example, you have the following file in the <code>cluster-deps</code> upstream project, e.g. aws-cluster.yaml.gotmpl.</p> <pre><code># ...\n\n## The machine pools configurations\nmachinePools:\n  app:\n    enabled: false\n    annotations: {}\n    labels: {}\n    managed:\n      spec:\n        # AdditionalTags is an optional set of tags to add to AWS resources managed by the AWS provider, in addition \n        # to the ones added by default.\n        additionalTags: {}\n        # DiskSize specifies the root disk size\n        diskSize: 10\n        # InstanceType specifies the AWS instance type\n        instanceType: t3.medium\n        # Labels specifies labels for the Kubernetes node objects\n        labels:\n          db: app\n        # Scaling specifies scaling for the ASG behind this pool\n        scaling:\n          maxSize: 1\n          minSize: 1\n        # Taints specifies the taints to apply to the nodes of the machine pool\n        taints:\n          - effect: NoSchedule\n            key: old-taint\n            value: value1\n        # ...\n    # Number of desired machines.\n    replicas: 1\n    spec:\n    # ...\n\n# ...\n</code></pre> <p>Then, if you need to change the <code>machinePools.app.*</code> values in the <code>develop</code> environment of the <code>rmk-test</code> downstream project, e.g., to enable the <code>app</code> nodes and increase their count to 10, you do not need to copy the entire file. Instead, you can override only the specific values by repeating their YAML path.</p> <p>Your override file will be minimalistic and contain only the necessary changes:</p> <pre><code>machinePools:\n  app:\n    enabled: true\n    managed:\n      spec:\n        scaling:\n          maxSize: 10\n          minSize: 10\n    replicas: 10\n</code></pre> <p>If you want to override the <code>taints</code> field, which is an array, the correct way to do this is to provide the whole array:</p> <pre><code>taints:\n  # old taint\n  - effect: NoSchedule\n    key: old-taint\n    value: value1\n  # new taint\n  - effect: PreferNoSchedule\n    key: new-taint\n    value: value2\n</code></pre> <p>You cannot override a subset of the array items: <pre><code>taints:\n  # incorrect\n  - effect: NoSchedule\n    key: new-taint\n    value: value2\n</code></pre></p> <p>To check the final result, run the rmk release template command and see the final YAML.</p>"},{"location":"configuration/release-management/release-management/#release-update-and-integration-into-the-cd-pipeline","title":"Release update and integration into the CD pipeline","text":"<p>The rmk release update command automates the process of updating and delivering releases according to the version changes of artifacts, e.g., container images, following the GitOps methodology.</p> <p>Since RMK is a binary file that can be downloaded and installed on any Unix-based operating system, it can be integrated with almost any CI/CD system: GitHub Actions, GitLab, Drone CI, Jenkins, etc.</p>"},{"location":"configuration/release-management/release-management/#example-of-integration-with-github-actions","title":"Example of integration with GitHub Actions","text":"<p>Prerequisites:</p> <ul> <li>The project repository has already     been generated and prepared using RMK.</li> </ul> <p>Create the following workflow in your project repository at <code>.github/workflows/release-update.yaml</code>. An example content of the GitHub Actions\u2019 workflow:</p> <pre><code>name: Release update\n\non:\n  workflow_dispatch:\n    inputs:\n      image_repository_full_name:\n        description: Image repository full name of application.\n        required: true\n      version:\n        description: Current application version.\n        required: true\n\njobs:\n  release-update:\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Checkout main repository\n        uses: actions/checkout@v4\n        with:\n          ref: ${{ github.ref }}\n          fetch-depth: 0\n\n      - name: Release update\n        env:\n          AWS_REGION: us-east-1\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          RMK_GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN_REPO }}\n          RMK_SLACK_WEBHOOK: ${{ secrets.RMK_SLACK_WEBHOOK }}\n          RMK_SLACK_CHANNEL: project1-cd-notifications\n          RMK_RELEASE_UPDATE_REPOSITORY: ${{ github.event.inputs.image_repository_full_name }}\n          RMK_RELEASE_UPDATE_TAG: ${{ github.event.inputs.version }}\n        run: |\n          curl -sL \"https://edenlabllc-rmk-tools-infra.s3.eu-north-1.amazonaws.com/rmk/s3-installer\" | bash\n\n          rmk config init --cluster-provider=aws --progress-bar=false --slack-notifications\n          rmk release update --skip-ci --deploy\n</code></pre> <p>In this example, we have prepared a <code>GitHub Action</code> that expects two input parameters:</p> <ul> <li><code>image_repository_full_name</code></li> <li><code>version</code></li> </ul> <p>As soon as a request with these parameters is sent to this action, RMK will be executed, first analyzing all the <code>releases.yaml</code> files to match the <code>image_repository_full_name</code> and will replace the tag field with the corresponding version if the versions differ. After that, it will automatically commit the changes to the current branch in the <code>releases.yaml</code> files where changes have been found. Then, it will synchronize the releases where the version changes were found.</p> <p>An example of the <code>releases.yaml</code> file:</p> <pre><code># ...\nfoo:\n  enabled: true\n  image:\n    repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/app.foo\n    tag: v0.1.0\nbar:\n  enabled: true\n  image:\n    repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/app.bar\n    tag: v0.1.1\n# ...\n</code></pre> <p>To fully automate code delivery in the CI pipeline, add a step that triggers the deployment after building and pushing the container image. This step should pass the full image repository name and tag via an API call using cURL or GitHub CLI. This ensures seamless deployment to the infrastructure environment.</p>"},{"location":"configuration/secrets-management/secrets-management/","title":"Secrets management","text":""},{"location":"configuration/secrets-management/secrets-management/#overview","title":"Overview","text":"<p>RMK utilizes SOPS and Age for secrets management, ensuring secure encryption, storage, and access to sensitive data. The tool ensures seamless and automated secret management, reducing manual effort while maintaining security best practices.</p> <p>The functionality in RMK is divided into two key areas:</p> <ol> <li>Working with secret keys: Managing encryption keys used for encrypting and decrypting secrets.</li> <li>Working with secret files: Handling encrypted YAML files that store sensitive configuration data.</li> </ol>"},{"location":"configuration/secrets-management/secrets-management/#secret-keys","title":"Secret keys","text":"<p>This area focuses on integration with cloud providers, ensuring secure storage, retrieval, and local access to secrets . Once the keys are generated, RMK automatically provisions all necessary cloud resources, securely stores the keys in the provider\u2019s Secrets Manager service, and downloads them to the local machine upon first use.</p> <p>For each supported cloud provider, RMK integrates with the respective secrets management service:</p> <ul> <li>AWS: Integrates with AWS Secrets Manager.</li> <li>Azure: Integrates with Azure Key Vault.</li> <li>GCP: Integrates   with Google Cloud Secret Manager.</li> </ul> <p>Locally, secret keys are stored in a secure file within the user\u2019s home directory:</p> <pre><code>${HOME}/.rmk/sops-age-keys/&lt;project_name&gt;/\n</code></pre> <p>For example, the directory for the <code>rmk-test</code> project:</p> <pre><code>${HOME}/.rmk/sops-age-keys/rmk-test/\n</code></pre> <p>might have the following content:</p> <ul> <li><code>.keys.txt</code>: the main merged file of all secret keys that SOPS uses.</li> <li><code>rmk-test-deps.txt</code>: secret key for the <code>deps</code> scope.</li> <li><code>rmk-test-rmk-test.txt</code>: secret key for the <code>rmk-test</code> scope.</li> </ul> <p>A secret key will look like this:</p> <pre><code># created: 2025-01-23T20:47:30+01:00\n# public key: age1rq0gx9zuwphw8kjx6ams84rgysqk5kdmhnysxs28r0x955xnzsdsslgtn0\nAGE-SECRET-KEY-15K8LZB3MT0QWJ4N7X90H2A9C5L6E7FYZ3XGKP1DRN8SWV2QXT90H2A9C5L\n</code></pre> <p>By design, each project and scope has its own key, ensuring isolated key storage.</p> <p>Secret keys are not separated by environment name. This allows secrets to be managed independently of the branch or environment currently in use.</p>"},{"location":"configuration/secrets-management/secrets-management/#secret-files","title":"Secret files","text":"<p>This area focuses on integration with Helmfile, Helm, and Kubernetes, ensuring automated and seamless secrets management throughout the deployment process.</p> <p>Normally, the secret files can be committed to Git because they are encrypted with secret keys using symmetric-key algorithms.</p> <p>In a project repository, all secret files are stored in the <code>etc/&lt;scope&gt;/&lt;environment&gt;/secrets/</code> directories. For example:</p> <pre><code>etc/deps/develop/secrets/postgres.yaml\netc/rmk-test/develop/secrets/app.yaml\n</code></pre> <p>Similar to the releases.yaml files, secrets files are never inherited by projects, in contrast to the Helmfile values. Each project should have its own unique set of secrets for all deployed releases.</p> <p>In the encrypted secret file, only the values are encrypted, while the object keys remain in plaintext. This approach allows teams to easily review changes in pull requests (PRs) without exposing sensitive information.</p> Example of an encrypted secret file, where sensitive values are encrypted using AES256-GCM <pre><code>username: ENC[AES256_GCM,data:A0jb1wU=,iv:RM8V1IOHvCrBv7f9f/GKobaBYyjcX9jcNQp6XPopNcU=,tag:T79VY3/yqlIffdbvYDwukQ==,type:str]\npassword: ENC[AES256_GCM,data:Kjo5hDSb+VmhdLLuq48oVg==,iv:5wpJBsiA5B82RRaguW8/TcKgGYbiZhihdIhXnPwyRG8=,tag:yQ5Chi949jBB1cSaFDVlOQ==,type:str]\nsops:\n  kms: []\n  gcp_kms: []\n  azure_kv: []\n  hc_vault: []\n  age:\n    - recipient: age1rq0gx9zuwphw8kjx6ams84rgysqk5kdmhnysxs28r0x955xnzsdsslgtn0\n      enc: |\n        -----BEGIN AGE ENCRYPTED FILE-----\n        YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSA1ZnBDR1pLNWt3TGVOVDlI\n        TGdNOEdDZmEzQjFzaWZuWXVqN0RZMWxBcjN3CnZvdDRtSDZIaTlyenF4bG9wRzg3\n        ZURpTGUrd3JIZGV6clpwTkVKeDT5ekkKLS0tIHUwMGVvWnFDY2FQWm8rcmg4Wnl3\n        YlJtb1dIczAvbnRNZWtqZlJLdXB5K1EKZHC1YAnMnRJdXfin1KYsbBZBViSysroo\n        8wLK53RXN4dgyLsLMmESAWqEqIGOgkns7gbP8N7efakI1aI239SlVg==\n        -----END AGE ENCRYPTED FILE-----\n  lastmodified: \"2025-01-25T09:40:29Z\"\n  mac: ENC[AES256_GCM,data:ytSnoJOi6eIzWjETgJo8/ppttKbHiSDcxQRJfocW0SWC2kQhyXtM0Y9R/d9JXbJrupqEcFH3yS4NJQz4uFyButI78pOrFxuhxNIhL3YSghTrBZKZ71IpjTe6W/oqz4UUhio5r1VU6KKFcKRKIvZZIUUnlqhJToOLB/VcLxqIQgw=,iv:Gufcas0JD7RVCTPIycN46EUq8V5OzYu++qmtolFu7hA=,tag:46k/pE546i4h18sXudp6Qw==,type:str]\n  pgp: []\n  unencrypted_suffix: _unencrypted\n  version: 3.7.1\n</code></pre> <p>The decrypted file will look like this:</p> <pre><code>username: user1\npassword: password1\n</code></pre> <p>This is the human-readable version of the secrets after RMK decrypts them using the appropriate secret key.</p>"},{"location":"configuration/secrets-management/secrets-management/#secret-keys-management","title":"Secret keys management","text":""},{"location":"configuration/secrets-management/secrets-management/#creating-secret-keys","title":"Creating secret keys","text":"<p>Initially, a Kubernetes cluster admin generates the required secret keys using the following command:</p> <pre><code>rmk secret keys create\n</code></pre> <p>Alternatively, the keys for all secret scopes can be generated using the following command:</p> <pre><code>rmk project generate --create-sops-age-keys\n</code></pre> <p>This command will:</p> <ul> <li>Generate a set of Age private keys for each scope.</li> <li>Store them in the user\u2019s home directory at:   <pre><code>${HOME}/.rmk/sops-age-keys/&lt;project_name&gt;/\n</code></pre></li> <li>Create an Age private key for each scope where an empty SOPS configuration file exists at:   <pre><code>etc/&lt;scope&gt;/&lt;environment&gt;/secrets/.sops.yaml\n</code></pre></li> <li>Automatically add <code>creation_rules</code> containing the public key and a <code>regex</code> for filtering secrets into   the <code>.sops.yaml</code> file.</li> </ul> <p>Example <code>.sops.yaml</code> configuration:</p> <pre><code>creation_rules:\n  - path_regex: .+\\.yaml$\n    age: 'age1rq0gx9zuwphw8kjx6ams84rgysqk5kdmhnysxs28r0x955xnzsdsslgtn0'\n</code></pre>"},{"location":"configuration/secrets-management/secrets-management/#uploading-secret-keys-to-a-remote-storage","title":"Uploading secret keys to a remote storage","text":"<p>After generating the keys, they can be explicitly uploaded to a remote secrets storage supported by the cloud provider:</p> <pre><code>rmk secret keys upload\n</code></pre> <p>Users must have the necessary permissions to upload  secret keys to the configured secrets storage service.</p>"},{"location":"configuration/secrets-management/secrets-management/#downloading-secret-keys-from-a-remote-storage","title":"Downloading secret keys from a remote storage","text":"<p>If the keys have already been created by another person (e.g., a cluster admin) and uploaded to the remote storage, users can download them to their local environment using the following command:</p> <pre><code>rmk secret keys download\n</code></pre> <p>Users must have read permissions to download keys. Without proper permissions, users won\u2019t be able to encode/decode secrets or release services using RMK.</p> <p>Once downloaded, the directory:</p> <pre><code>${HOME}/.rmk/sops-age-keys/&lt;project_name&gt;/\n</code></pre> <p>will contain all the necessary keys for secrets encryption and decryption.</p>"},{"location":"configuration/secrets-management/secrets-management/#batch-secrets-management","title":"Batch secrets management","text":""},{"location":"configuration/secrets-management/secrets-management/#overview_1","title":"Overview","text":"<p>RMK secrets manager automates secret management in Kubernetes in batch mode.</p> <p>Key features include:</p> <ul> <li>Batch secret generation and encryption using Golang templates   and Sprig.</li> <li>Flexible configuration and process separation for project, scope, and environment levels.</li> </ul>"},{"location":"configuration/secrets-management/secrets-management/#generating-all-secrets-from-scratch","title":"Generating all secrets from scratch","text":"<p>When creating a new project from scratch, all required directories, such as:</p> <pre><code>etc/&lt;scope&gt;/&lt;environment&gt;/secrets/\n</code></pre> <p>must first include an empty <code>.sops.yaml</code> file with no content. This file acts as an indicator that secrets will be managed by RMK for this scope and environment.</p> <p>To create an empty file in the specified directory, the following commands can be utilized:</p> <pre><code>mkdir -p etc/&lt;scope&gt;/&lt;environment&gt;/secrets\ntouch etc/&lt;scope&gt;/&lt;environment&gt;/secrets/.spec.yaml.gotmpl\n</code></pre> <p>For example:</p> <pre><code>mkdir -p etc/deps/develop/secrets\ntouch etc/deps/develop/secrets/.spec.yaml.gotmpl\n</code></pre> <p>This ensures that the required directory structure exists before generating secrets.</p> <p>After that, each scope requires a <code>.spec.yaml.gotmpl</code> template file to define the structure of the generated secrets. This file is processed using the Sprig templating engine. In addition to the standard functions provided by the engine, RMK extends <code>.spec.yaml.gotmpl</code> with the following extra template functions:</p> <ul> <li><code>{{ requiredEnv \"VAR_NAME\" }}</code> \u2013 requires the specified environment variable as input.</li> <li><code>{{ prompt \"VAR_NAME\" }}</code> \u2013 prompts the user for interactive input.</li> </ul> Example <code>.spec.yaml.gotmpl</code> file <pre><code>generation-rules:\n  - name: email-sender\n    template: |\n      envSecret:\n        EMAIL_API_KEY: {{ requiredEnv \"EMAIL_API_KEY\" }}\n        EMAIL_SENDER: {{ prompt \"EMAIL_SENDER\" }}\n  - name: postgres\n    template: |\n      rootUsername: root\n      rootPassword: {{ randAlphaNum 16 }}\n      appUsername: {{ requiredEnv \"POSTGRES_USERNAME\" }}\n      appPassword: {{ prompt \"POSTGRES_PASSWORD\" }}\n  - name: redis\n    template: |\n      auth:\n        password: {{ randAlphaNum 16 }}\n      cacheTTL: {{ requiredEnv \"REDIS_TTL\" }}\n</code></pre>  In this example:   <ul> <li>The <code>name</code> field corresponds to a Helmfile/Helm release name, such as <code>email-sender</code>, <code>postgres</code>, or <code>redis</code>.</li> <li><code>EMAIL_API_KEY</code>, <code>POSTGRES_USER</code>, and <code>REDIS_TTL</code> must be set as environment variables before running the generation process.</li> <li>The user is prompted to enter the email sender address (<code>EMAIL_SENDER</code>) and the PostgreSQL application password (<code>POSTGRES_PASSWORD</code>) interactively.</li> </ul> <p>All variables defined in the template using the <code>requiredEnv</code> function must be exported before executing the <code>rmk secret manager generate</code> command.</p> <p>In our example, the required variables are (<code>EMAIL_API_KEY</code>, <code>POSTGRES_USER</code>, <code>REDIS_TTL</code>). They can be exported as follows:</p> <pre><code>export EMAIL_API_KEY=\"dummy-email-api-key-XXX\"\nexport POSTGRES_USER=\"user1\"\nexport REDIS_TTL=\"3600\"\n</code></pre> <p>After exporting the variables, run the following command to generate all secret files in batch mode:</p> <pre><code>rmk secret manager generate\n</code></pre> <p>For the example above, secret files will be created for each of the three releases (<code>email-sender</code>, <code>postgres</code>, and <code>redis</code>). These files will be generated in the <code>etc/deps/develop/secrets</code> directory with the following naming pattern:</p> <pre><code>etc/deps/develop/secrets/email-sender.yaml\netc/deps/develop/secrets/postgres.yaml\netc/deps/develop/secrets/redis.yaml\n</code></pre> <p>The secrets generation process runs in an idempotent mode, skipping previously generated files and logging a warning if they already exist.</p> <p>Each secret file will be generated in plaintext YAML format and should be reviewed before encryption and committing to Git.</p>"},{"location":"configuration/secrets-management/secrets-management/#encrypt-all-the-generated-secrets","title":"Encrypt all the generated secrets","text":"<p>Once the secrets have been verified, encrypt them using:</p> <pre><code>rmk secret manager encrypt\n</code></pre> <p>Directories that do not contain a <code>.sops.yaml</code> or <code>.spec.yaml.gotmpl</code> file will be ignored.</p> <p>Additionally, each <code>.sops.yaml</code> file will be automatically updated with the correct paths and the public keys of the secret keys used for encryption.</p> <p>Manual editing of the encrypted secrets files is strictly forbidden, because SOPS automatically controls the checksums of the secret files. To safely modify encrypted secrets, always use the specialized edit command.</p>"},{"location":"configuration/secrets-management/secrets-management/#create-a-new-secret-later","title":"Create a new secret later","text":"<p>To generate and encode a new secret in addition to the previously generated ones (e.g., when a new service (release) is added), a template for the new secret should be added to <code>.spec.yaml.gotmpl</code>.</p> <p>For example, for a new <code>new-app</code> release:</p> <pre><code>generation-rules:\n  # ...\n  - name: new-app\n    template: |\n      username: {{ requiredEnv \"APP_USERNAME\" }}\n      password: {{ requiredEnv \"APP_PASSWORD\" }}\n# ...\n</code></pre> <p>Then, generate the new secret as a plain YAML file and encrypt it using RMK for the required scope and environment.</p> <p>For example, for the <code>rmk-test</code> scope and <code>develop</code> environment:</p> <pre><code>export APP_USERNAME=\"user1\"\nexport APP_PASSWORD=\"password1\"\nrmk secret manager generate --scope rmk-test --environment develop\nrmk secret manager encrypt --scope rmk-test --environment develop\n</code></pre> <p>Finally, a secret file will be created for the <code>new-app</code> release in the <code>etc/deps/develop/secrets</code> directory:</p> <pre><code>etc/deps/develop/secrets/new-app.yaml\n</code></pre>"},{"location":"configuration/secrets-management/secrets-management/#rotating-all-the-secrets-for-a-specific-scope-and-environment","title":"Rotating all the secrets for a specific scope and environment","text":"<p>To regenerate all the secrets for a specific scope and environment (e.g., when existing secrets have been compromised and need to be replaced) based on the <code>.spec.yaml.gotmpl</code> file, use the <code>--force</code> flag. This ensures that previously generated secret files are overwritten.</p> <p>For example, to rotate secrets for the <code>rmk-test</code> scope in the <code>production</code> environment, run:</p> <pre><code># Export all required environment variables before generating\nrmk secret manager generate --scope rmk-test --environment production --force\nrmk secret manager encrypt --scope rmk-test --environment production\n</code></pre> <p>This process ensures that all secrets are freshly generated and securely encrypted before deployment.</p>"},{"location":"configuration/secrets-management/secrets-management/#working-with-a-single-secret","title":"Working with a single secret","text":"<p>All RMK commands related to the secrets management can be found under the rmk secret command category.</p>"},{"location":"configuration/secrets-management/secrets-management/#creating-or-editing-a-secret","title":"Creating or editing a secret","text":"<p>The <code>rmk secret edit</code> command operates in an idempotent mode, meaning it can be used for both creating new secrets (e.g., when adding a new release) and modifying existing ones.</p> <p>To create or edit a secret, run:</p> <pre><code>rmk secret edit &lt;path_to_new_file_or_existing_secret&gt;\n</code></pre> <p>For example:</p> <pre><code>rmk secret edit etc/deps/develop/secrets/postgres.yaml\n</code></pre> <p>This command will open a CLI text editor (e.g., vim). After making the necessary changes, save and exit the editor. The updated secret will be automatically encrypted, so no additional encoding is required.</p> <p>Manual editing of the encrypted secrets files is strictly forbidden, because SOPS automatically controls the checksums of the secret files.</p>"},{"location":"configuration/secrets-management/secrets-management/#viewing-an-existing-secret","title":"Viewing an existing secret","text":"<p>To view the decrypted content of an existing secret, use:</p> <pre><code>rmk secret view &lt;path_to_existing_secret&gt;\n</code></pre> <p>For example:</p> <pre><code>rmk secret view etc/deps/develop/secrets/postgres.yaml\n</code></pre> <p>This is useful for inspecting credentials of deployed services, such as database access details or authentication credentials for a web UI.</p>"}]}